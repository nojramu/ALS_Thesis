{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9ml01UjABN56",
        "JoEP5gBfCEJn",
        "H1g-YrGGCUb9",
        "-4NgHJz2Cylu",
        "beZ4XVviC6S6",
        "VFjEmG3SDJUy",
        "wu48g9BPDbum",
        "vWWH_MIPECgM",
        "64a4223f",
        "6fdd6ef5",
        "mNe0hHZXIB4Y",
        "R4NS0t0nI8wP",
        "lus3BNyzJ1oU",
        "5da90596",
        "LHH2Aq71U_KU",
        "3UG_6FmqqVAq",
        "E_s-ZWjAu5q2",
        "n7ybYZjNllag",
        "PlMA-pWltrFd",
        "b_X-Jgj-sV7l",
        "l3VhX2mQx9SB",
        "c159bb76",
        "aaae0769",
        "d4a1d3e2",
        "eca2afa2",
        "c48672e9"
      ],
      "mount_file_id": "1ibJwOcjExcq3tKWJE9QHfbMKNXH47td_",
      "authorship_tag": "ABX9TyOhYwtAMRBQEKt8NKPB1UGp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nojramu/ALS_Thesis/blob/main/ALS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adaptive Learning System\n",
        "Made by: Engr. Marjon D. Umbay  \n",
        "Created for paper article in compliance of the subject of:  \n",
        "* Numerical Methods and Techniques  \n",
        "* Technopreneurship and Innovation"
      ],
      "metadata": {
        "id": "JirFQW0ktAM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Regression and Classification\n",
        "\n",
        "This section focuses on using Random Forest models to predict two key aspects of the adaptive learning system: cognitive load and engagement level.\n",
        "\n",
        "*   **Random Forest Regressor**: A machine learning model used here to predict a continuous value, specifically the estimated cognitive load of the learner.\n",
        "*   **Random Forest Classifier**: A machine learning model used to predict a categorical value, in this case, the learner's engagement level (discretized into different levels).\n",
        "\n",
        "The process involves loading and preprocessing data, splitting it for training and testing, training the two Random Forest models independently, and then evaluating their performance. The trained models are intended to be used later in the system to provide inputs for the decision-making process."
      ],
      "metadata": {
        "id": "9ml01UjABN56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprosessor of Data"
      ],
      "metadata": {
        "id": "JoEP5gBfCEJn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e24317f5"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_and_preprocess_data(csv_file_path=None, data=None):\n",
        "    \"\"\"\n",
        "    Loads data from a CSV file or accepts a DataFrame and performs preprocessing,\n",
        "    including initial cleaning.\n",
        "\n",
        "    Args:\n",
        "      csv_file_path (str, optional): The path to the CSV file containing the data.\n",
        "                                     If None, 'data' must be provided.\n",
        "      data (pd.DataFrame, optional): A DataFrame containing the data.\n",
        "                                   If None, 'csv_file_path' must be provided.\n",
        "\n",
        "    Returns:\n",
        "      Cleaned and preprocessed pandas DataFrame, or None if an error occurs,\n",
        "      required columns are missing, or both csv_file_path and data are None.\n",
        "    \"\"\"\n",
        "    df = None\n",
        "    if csv_file_path:\n",
        "        # Load the data from CSV\n",
        "        try:\n",
        "            df = pd.read_csv(csv_file_path)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: File not found at {csv_file_path}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading CSV file: {e}\")\n",
        "            return None\n",
        "    elif data is not None:\n",
        "        # Use the provided DataFrame\n",
        "        df = data.copy() # Work on a copy to avoid modifying the original\n",
        "    else:\n",
        "        print(\"Error: Either csv_file_path or data must be provided.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "    if df is None:\n",
        "        return None\n",
        "\n",
        "    # Check if required columns exist\n",
        "    required_features = ['engagement_rate', 'time_on_task_s', 'hint_ratio', 'interaction_count',\n",
        "                         'task_completed', 'quiz_score', 'difficulty', 'error_rate',\n",
        "                         'task_timed_out', 'time_before_hint_used']\n",
        "\n",
        "    # If it's training data, also check for target columns\n",
        "    is_training_data = 'engagement_level' in df.columns and 'cognitive_load' in df.columns\n",
        "    if is_training_data:\n",
        "         required_cols = required_features + ['engagement_level', 'cognitive_load']\n",
        "    else:\n",
        "         required_cols = required_features # Only features are required for new data\n",
        "\n",
        "\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "        print(f\"Error: Missing required columns in the data: {missing_cols}\")\n",
        "        return None\n",
        "\n",
        "    # Preprocess the data: Convert to numeric and fill missing values\n",
        "    # Apply to columns expected to be numeric\n",
        "    numeric_cols_to_fill = [\n",
        "        'engagement_rate', 'time_on_task_s', 'hint_ratio', 'interaction_count',\n",
        "        'quiz_score', 'difficulty', 'error_rate', 'time_before_hint_used'\n",
        "    ]\n",
        "    if is_training_data:\n",
        "        numeric_cols_to_fill.append('cognitive_load') # Include cognitive_load for training data\n",
        "\n",
        "\n",
        "    for col in numeric_cols_to_fill:\n",
        "         if col in df.columns: # Check if column exists before processing\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "            df[col] = df[col].fillna(df[col].median()) # Use median for robustness\n",
        "\n",
        "    # Convert boolean/integer columns to integer (if they exist and are not already numeric)\n",
        "    int_cols = ['task_completed', 'task_timed_out']\n",
        "    if is_training_data:\n",
        "        int_cols.append('engagement_level') # Include engagement_level for training data\n",
        "\n",
        "    for col in int_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "            if col in ['task_completed', 'task_timed_out']:\n",
        "                 df[col] = df[col].fillna(0).astype(int)\n",
        "            elif col == 'engagement_level':\n",
        "                 df[col] = df[col].fillna(df[col].median()).astype(int)\n",
        "\n",
        "    return df"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest Training"
      ],
      "metadata": {
        "id": "H1g-YrGGCUb9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3070fdf"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "def train_cognitive_and_engagement_models(csv_file_path, test_set_size=0.2, random_state_value=20, n_estimators_value=100):\n",
        "  \"\"\"\n",
        "  Trains Random Forest models for predicting cognitive load and engagement level\n",
        "  from a CSV file.\n",
        "\n",
        "  Args:\n",
        "    csv_file_path (str): The path to the CSV file containing the data.\n",
        "    test_set_size (float, optional): The proportion of the dataset to include in the test split. Defaults to 0.2.\n",
        "    random_state_value (int, optional): Controls the shuffling applied to the data before applying the split.\n",
        "    n_estimators_value (int, optional): The number of trees in the forest.\n",
        "\n",
        "  Returns:\n",
        "    tuple: A tuple containing two trained models:\n",
        "           - rf_cognitive_load (RandomForestRegressor): Trained model for predicting cognitive load.\n",
        "           - rf_engagement_level (RandomForestClassifier): Trained model for predicting engagement level.\n",
        "           Returns (None, None) if data loading or preprocessing fails.\n",
        "    dict: A dictionary containing the feature names used for training.\n",
        "  \"\"\"\n",
        "\n",
        "  # Load and preprocess the data using the unified function\n",
        "  # Assuming load_and_preprocess_data function is available in the notebook environment or imported\n",
        "  try:\n",
        "      # You might need to define or import load_and_preprocess_data first if it's not in this cell\n",
        "      # For now, we'll assume it returns a DataFrame similar to the original structure but processed\n",
        "      # with 'cognitive_load', 'engagement_level', and features like 'difficulty' (now numeric)\n",
        "      df_processed = load_and_preprocess_data(csv_file_path=csv_file_path)\n",
        "  except NameError:\n",
        "      print(\"Error: 'load_and_preprocess_data' function not found. Please ensure it's defined or imported.\")\n",
        "      return None, None, None\n",
        "  except Exception as e:\n",
        "      print(f\"Error loading or preprocessing data: {e}\")\n",
        "      return None, None, None\n",
        "\n",
        "\n",
        "  if df_processed is None:\n",
        "      print(\"Preprocessing returned None.\")\n",
        "      return None, None, None # Return None for models and features\n",
        "\n",
        "  # Define features (X) and targets (y) - 'difficulty' is now a numeric feature\n",
        "  target_cols = ['cognitive_load', 'engagement_level']\n",
        "  # Ensure 'difficulty' is included in features if it exists after processing\n",
        "  # Assuming the processed dataframe includes the features needed for training\n",
        "  features = [col for col in df_processed.columns if col not in target_cols]\n",
        "\n",
        "  # Ensure all feature columns exist after preprocessing\n",
        "  # Note: This check is redundant if load_and_preprocess_data already checked required_features\n",
        "  # but keeping for safety after feature definition.\n",
        "  # This also assumes load_and_preprocess_data correctly processes the input CSV\n",
        "  # to produce features like 'simpsons_integral_level', 'engagement_level', 'task_completed', 'prev_task_type'\n",
        "  # and potentially others derived from the raw data.\n",
        "  # A more robust approach would explicitly define expected input features and output features of preprocessing.\n",
        "  # For the scope of this function, we assume df_processed is ready for model training.\n",
        "\n",
        "  X = df_processed[features]\n",
        "  y_cognitive_load = df_processed['cognitive_load']\n",
        "  y_engagement_level = df_processed['engagement_level']\n",
        "\n",
        "  # --- IMPROVEMENT: Address Model Evaluation ---\n",
        "  # The previous implementation trained the final models on the *entire* dataset (X, y)\n",
        "  # but reported evaluation metrics calculated on the test split. This leads to\n",
        "  # misleadingly optimistic metrics.\n",
        "\n",
        "  # Option A: Train final models on X_train and evaluate *only* on X_test.\n",
        "  # This provides a more realistic estimate of generalization performance.\n",
        "  # If the goal is to deploy a model trained on all available data, Option B is better.\n",
        "\n",
        "  # Option B: Train final models on the full dataset (X, y) for deployment,\n",
        "  # but report evaluation metrics from the test split *before* full-data training,\n",
        "  # or clearly document that test metrics are for development insights only.\n",
        "  # This matches the *structure* of the original code more closely, but we will\n",
        "  # clarify the interpretation.\n",
        "\n",
        "  # We will keep the structure of training on the full dataset (X, y) for the final models\n",
        "  # but clarify the evaluation metrics' purpose in comments or surrounding text.\n",
        "  # The evaluation metrics printed will still be from the *separate* test split (X_test, y_test)\n",
        "  # which was created *before* training the final models on the full dataset.\n",
        "\n",
        "  # Split data into training and testing sets for evaluation purposes\n",
        "  X_train_eval, X_test_eval, y_cognitive_load_train_eval, y_cognitive_load_test_eval = train_test_split(\n",
        "      X, y_cognitive_load, test_size=test_set_size, random_state=random_state_value)\n",
        "  # Split for engagement level - ensure stratification if it's a classification problem and classes are imbalanced\n",
        "  X_train_eval, X_test_eval, y_engagement_level_train_eval, y_engagement_level_test_eval = train_test_split(\n",
        "      X, y_engagement_level, test_size=test_set_size, random_state=random_state_value, stratify=y_engagement_level)\n",
        "\n",
        "\n",
        "  # Initialize and train the Random Forest Regressor for cognitive load\n",
        "  # Train the FINAL model on the entire dataset for potentially better performance in deployment\n",
        "  rf_cognitive_load = RandomForestRegressor(n_estimators=n_estimators_value, random_state=random_state_value)\n",
        "  rf_cognitive_load.fit(X, y_cognitive_load)\n",
        "\n",
        "  # Initialize and train the Random Forest Classifier for engagement level\n",
        "  # Train the FINAL model on the entire dataset\n",
        "  rf_engagement_level = RandomForestClassifier(n_estimators=n_estimators_value, random_state=random_state_value)\n",
        "  rf_engagement_level.fit(X, y_engagement_level)\n",
        "\n",
        "\n",
        "  # Evaluate the models on the test set *created before* training on the full dataset\n",
        "  # These metrics provide an estimate of performance but may be optimistic\n",
        "  # compared to truly unseen data if the full dataset was used for training.\n",
        "  print(\"\\n--- Model Evaluation (Metrics on Test Split before full-data training) ---\")\n",
        "  cognitive_load_predictions_eval = rf_cognitive_load.predict(X_test_eval)\n",
        "  print(f\"Cognitive Load MSE on test split: {mean_squared_error(y_cognitive_load_test_eval, cognitive_load_predictions_eval):.4f}\")\n",
        "\n",
        "  engagement_level_predictions_eval = rf_engagement_level.predict(X_test_eval)\n",
        "  print(f\"Engagement Level Accuracy on test split: {accuracy_score(y_engagement_level_test_eval, engagement_level_predictions_eval):.4f}\")\n",
        "\n",
        "\n",
        "  # Return trained models (trained on full data) and the list of features used for training\n",
        "  # Add a comment indicating that these models were trained on the full dataset\n",
        "  print(\"\\nRandom Forest models trained on the full dataset.\")\n",
        "  return rf_cognitive_load, rf_engagement_level, features # Models trained on full X, y"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Tester"
      ],
      "metadata": {
        "id": "-4NgHJz2Cylu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file_path = '/content/drive/MyDrive/The Paper/Numerical/Code/training_data_v2.csv'\n",
        "train_cognitive_and_engagement_models(csv_file_path, 0.2, 20, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hXJOW55h3n5",
        "outputId": "55a5c902-8c6e-4591-bf47-92daba7e4130"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Model Evaluation (Metrics on Test Split before full-data training) ---\n",
            "Cognitive Load MSE on test split: 241.2075\n",
            "Engagement Level Accuracy on test split: 1.0000\n",
            "\n",
            "Random Forest models trained on the full dataset.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(RandomForestRegressor(random_state=20),\n",
              " RandomForestClassifier(random_state=20),\n",
              " ['engagement_rate',\n",
              "  'time_on_task_s',\n",
              "  'hint_ratio',\n",
              "  'interaction_count',\n",
              "  'task_completed',\n",
              "  'quiz_score',\n",
              "  'difficulty',\n",
              "  'error_rate',\n",
              "  'task_timed_out',\n",
              "  'time_before_hint_used'])"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest Predictor"
      ],
      "metadata": {
        "id": "beZ4XVviC6S6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d8b3306"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def predict_cognitive_load_and_engagement(models, feature_names, new_data_path=None, new_data_df=None, new_data_list=None):\n",
        "  \"\"\"\n",
        "  Makes predictions for cognitive load and engagement level using trained models.\n",
        "\n",
        "  Args:\n",
        "    models (tuple): A tuple containing the trained cognitive load regressor\n",
        "                    and engagement level classifier (returned by train_cognitive_and_engagement_models).\n",
        "    feature_names (list): A list of the feature names the models were trained on.\n",
        "                          This is returned by train_cognitive_and_engagement_models.\n",
        "    new_data_path (str, optional): The path to a CSV file containing the new data for prediction.\n",
        "                                   Either new_data_path, new_data_df, or new_data_list must be provided.\n",
        "    new_data_df (pd.DataFrame, optional): A DataFrame containing the new data for prediction.\n",
        "                                        Either new_data_path, new_data_df, or new_data_list must be provided.\n",
        "    new_data_list (list, optional): A list of values representing a single data point for prediction.\n",
        "                                    The order of values should match the training features.\n",
        "                                    Either new_data_path, new_data_df, or new_data_list must be provided.\n",
        "\n",
        "  Returns:\n",
        "    tuple: A tuple containing:\n",
        "           - predicted_cognitive_load (np.array): Predicted cognitive load values.\n",
        "           - predicted_engagement_level (np.array): Predicted engagement level values.\n",
        "           Returns (None, None) if models are not provided or data is invalid.\n",
        "  \"\"\"\n",
        "  if models is None or len(models) != 2 or feature_names is None:\n",
        "    print(\"Error: Invalid models or feature names provided.\")\n",
        "    return None, None\n",
        "\n",
        "  if new_data_path is None and new_data_df is None and new_data_list is None:\n",
        "      print(\"Error: Either new_data_path, new_data_df, or new_data_list must be provided.\")\n",
        "      return None, None\n",
        "\n",
        "  rf_cognitive_load, rf_engagement_level = models\n",
        "  train_features = feature_names # Use feature_names passed from training\n",
        "\n",
        "  if new_data_list is not None:\n",
        "      if len(new_data_list) != len(train_features):\n",
        "          print(f\"Error: The number of values in new_data_list ({len(new_data_list)}) does not match the number of training features ({len(train_features)}).\")\n",
        "          return None, None\n",
        "      # Create a DataFrame from the list, using the training feature names as columns\n",
        "      # Ensure correct data types here if possible, though load_and_preprocess_data handles conversion\n",
        "      new_data_for_pred = pd.DataFrame([new_data_list], columns=train_features)\n",
        "\n",
        "  elif new_data_path is not None:\n",
        "      # Use the unified load_and_preprocess_data function\n",
        "      new_data_for_pred = load_and_preprocess_data(csv_file_path=new_data_path)\n",
        "      if new_data_for_pred is None:\n",
        "           print(f\"Error loading or preprocessing data from {new_data_path}\")\n",
        "           return None, None\n",
        "\n",
        "  elif new_data_df is not None:\n",
        "      new_data_for_pred = new_data_df.copy() # Work on a copy\n",
        "\n",
        "  else:\n",
        "      return None, None # Should not happen based on checks above\n",
        "\n",
        "\n",
        "  # Ensure new_data_for_pred has the same columns as the training data\n",
        "  # Add missing columns from training data with a value of 0\n",
        "  for col in train_features:\n",
        "      if col not in new_data_for_pred.columns:\n",
        "          new_data_for_pred[col] = 0\n",
        "\n",
        "  # Ensure columns are in the same order as the training data\n",
        "  # This is crucial for consistent predictions\n",
        "  try:\n",
        "      new_data_for_pred = new_data_for_pred[train_features]\n",
        "  except KeyError as e:\n",
        "      print(f\"Error: Feature '{e}' from training data not found in preprocessed new data.\")\n",
        "      # This might happen if a required original column was missing in the new_data input\n",
        "      return None, None\n",
        "\n",
        "\n",
        "  try:\n",
        "      predicted_cognitive_load = rf_cognitive_load.predict(new_data_for_pred)\n",
        "      predicted_engagement_level = rf_engagement_level.predict(new_data_for_pred)\n",
        "      return predicted_cognitive_load, predicted_engagement_level\n",
        "  except Exception as e:\n",
        "      print(f\"Error during prediction: {e}\")\n",
        "      return None, None"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction Tester"
      ],
      "metadata": {
        "id": "VFjEmG3SDJUy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFoMdNZoOr2f",
        "outputId": "a8af7e11-1376-414c-adeb-ff20e4436e68"
      },
      "source": [
        "# Define the path to your CSV file (Assuming you have mounted Google Drive and know the path)\n",
        "# Replace with the actual path to your CSV file if it's different\n",
        "csv_file_path = '/content/drive/MyDrive/The Paper/Numerical/Code/training_data_v2.csv'\n",
        "\n",
        "# Train the models and capture the returned models and feature names\n",
        "# Updated to unpack 3 values as returned by the function\n",
        "trained_cognitive_model, trained_engagement_model, trained_feature_names = train_cognitive_and_engagement_models(csv_file_path)\n",
        "\n",
        "# Combine the two trained models into a tuple to pass to the predict function\n",
        "trained_models = (trained_cognitive_model, trained_engagement_model)\n",
        "\n",
        "\n",
        "# Example new data for prediction as a DataFrame with original columns, including 'difficulty' as a number (0-10)\n",
        "# The load_and_preprocess_data function will handle the conversion to numeric and filling missing values.\n",
        "new_data_point_original_df = pd.DataFrame({\n",
        "    'engagement_rate': [0.85], # 0-1\n",
        "    'time_on_task_s': [501], # seconds\n",
        "    'hint_ratio': [0.67], # 0-1\n",
        "    'interaction_count': [14],\n",
        "    'task_completed': [0],  # Can be int (0/1) or boolean (False/True)\n",
        "    'quiz_score': [89.11], # 0-100\n",
        "    'difficulty': [2], # Provide the difficulty as a number between 0-10\n",
        "    'error_rate': [0.59], # 0-1\n",
        "    'task_timed_out': [0], # Can be int (0/1) or boolean (False/True)\n",
        "    'time_before_hint_used': [199] # The longer the better\n",
        "})\n",
        "\n",
        "# Make predictions using the original new data DataFrame and the captured models and feature names\n",
        "if trained_models is not None and trained_feature_names is not None:\n",
        "    predicted_cognitive_load, predicted_engagement_level = predict_cognitive_load_and_engagement(\n",
        "        models=trained_models,\n",
        "        feature_names=trained_feature_names,\n",
        "        new_data_df=new_data_point_original_df # Pass the original new data DataFrame\n",
        "    )\n",
        "\n",
        "    if predicted_cognitive_load is not None and predicted_engagement_level is not None:\n",
        "        print(\"Predicted Cognitive Load:\", predicted_cognitive_load)\n",
        "        print(\"Predicted Engagement Level:\", predicted_engagement_level)\n",
        "    else:\n",
        "        print(\"Prediction failed.\")\n",
        "else:\n",
        "    print(\"Models are not trained. Please check the training process.\")"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Model Evaluation (Metrics on Test Split before full-data training) ---\n",
            "Cognitive Load MSE on test split: 241.2075\n",
            "Engagement Level Accuracy on test split: 1.0000\n",
            "\n",
            "Random Forest models trained on the full dataset.\n",
            "Predicted Cognitive Load: [70.20109]\n",
            "Predicted Engagement Level: [5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kalman's Filter\n",
        "\n",
        "This section implements and applies a Kalman filter, a powerful algorithm used for estimating the state of a dynamic system from a series of noisy measurements. In this context, it's used to smooth the predicted cognitive load values obtained from the Random Forest Regressor.\n",
        "\n",
        "*   **Purpose**: To reduce noise and provide a more stable and accurate estimate of the learner's cognitive load over time.\n",
        "*   **How it works**: The filter uses a two-step process: prediction (forecasting the next state) and update (correcting the prediction based on the actual measurement). It maintains an estimate of the state and its uncertainty (covariance).\n",
        "\n",
        "By applying the Kalman filter to the sequence of cognitive load predictions, we aim to get a smoother trend that is less susceptible to short-term fluctuations, providing a potentially better input for subsequent decision-making processes like the Q-Learning engine."
      ],
      "metadata": {
        "id": "wu48g9BPDbum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Sample Predictions"
      ],
      "metadata": {
        "id": "vWWH_MIPECgM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5403b32a",
        "outputId": "0bf01418-7267-4dd7-fd39-1383ccffda97"
      },
      "source": [
        "import pandas as pd\n",
        "csv_file_path2 = '/content/drive/MyDrive/The Paper/Numerical/Code/sample_predictions.csv'\n",
        "try:\n",
        "    df_predictions = pd.read_csv(csv_file_path2)\n",
        "    display(df_predictions.head())\n",
        "    df_predictions.info()\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {csv_file_path2}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading CSV file: {e}\")"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   cognitive_load  engagement_level\n",
              "0         75.8690                 4\n",
              "1         75.2602                 5\n",
              "2         86.8593                 5\n",
              "3         80.7670                 5\n",
              "4         80.0618                 4"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d51f07e9-628d-4dd0-826d-522d71a5b691\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cognitive_load</th>\n",
              "      <th>engagement_level</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>75.8690</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>75.2602</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>86.8593</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>80.7670</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>80.0618</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d51f07e9-628d-4dd0-826d-522d71a5b691')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d51f07e9-628d-4dd0-826d-522d71a5b691 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d51f07e9-628d-4dd0-826d-522d71a5b691');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d32f8930-94c6-40cb-9d55-0f51190ef812\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d32f8930-94c6-40cb-9d55-0f51190ef812')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d32f8930-94c6-40cb-9d55-0f51190ef812 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(f\\\"Error loading CSV file: {e}\\\")\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"cognitive_load\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.6608264801427675,\n        \"min\": 75.2602,\n        \"max\": 86.8593,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          75.2602,\n          80.0618,\n          86.8593\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"engagement_level\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 4,\n        \"max\": 5,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          5,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 2 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   cognitive_load    100 non-null    float64\n",
            " 1   engagement_level  100 non-null    int64  \n",
            "dtypes: float64(1), int64(1)\n",
            "memory usage: 1.7 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64a4223f"
      },
      "source": [
        "### Plot Cognitive Load by Row Number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "544d886b",
        "outputId": "11357203-09b6-4620-dde3-b29622c4d9fc"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "# Create a Plotly figure object\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add a scatter trace for the original 'cognitive_load' data\n",
        "fig.add_trace(go.Scatter(x=df_predictions.index, y=df_predictions['cognitive_load'],\n",
        "                         mode='lines', name='Cognitive Load'))\n",
        "\n",
        "# Update the layout with title and axis labels\n",
        "fig.update_layout(\n",
        "    title='Cognitive Load by Row',\n",
        "    xaxis_title='Row',\n",
        "    yaxis_title='Cognitive Load'\n",
        ")\n",
        "\n",
        "# Display the Plotly figure\n",
        "fig.show()"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"f6570344-8e18-4ef4-a56f-3d0b22839482\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f6570344-8e18-4ef4-a56f-3d0b22839482\")) {                    Plotly.newPlot(                        \"f6570344-8e18-4ef4-a56f-3d0b22839482\",                        [{\"mode\":\"lines\",\"name\":\"Cognitive Load\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99],\"y\":[75.869,75.2602,86.8593,80.767,80.0618,71.347,88.0693,81.5807,82.1533,89.1997,95.3333,100.0,84.8895,93.3333,93.104,89.618,86.794,75.6203,90.2763,78.1565,87.3333,82.306,81.2125,82.6207,75.72,85.783,75.339,76.4922,84.2548,78.7903,78.6667,66.2507,76.5057,76.5172,91.9173,82.1612,95.8282,98.2683,99.331,91.3972,98.8702,93.9178,100.0,90.5048,92.0442,91.3203,88.5202,99.0107,94.3583,92.5117,97.3647,86.5985,91.6882,100.0,90.988,72.9095,83.1815,85.8997,72.5815,79.8448,78.6667,62.994,73.0757,62.1028,72.0,75.3643,77.8432,75.187,83.5122,82.0,78.9677,87.2215,75.8163,85.3333,86.3698,83.3195,99.9907,94.0,80.2258,100.0,90.7135,91.5147,81.6462,74.4538,77.0672,75.5395,78.0,60.3732,55.6593,49.4583,64.6667,57.0098,60.5355,54.4432,46.067,50.525,49.0383,72.6667,68.8353,67.4218],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Cognitive Load by Row\"},\"xaxis\":{\"title\":{\"text\":\"Row\"}},\"yaxis\":{\"title\":{\"text\":\"Cognitive Load\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f6570344-8e18-4ef4-a56f-3d0b22839482');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fdd6ef5"
      },
      "source": [
        "### Kalman Filter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "456d42c2"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def apply_kalman_filter(measurements, initial_covariance=1000.0, process_noise=0.1, measurement_noise=1.0):\n",
        "    \"\"\"\n",
        "    Applies a Kalman filter to a sequence of measurements with adjustable parameters.\n",
        "\n",
        "    Args:\n",
        "        measurements (np.array): A 1D numpy array of measurements.\n",
        "        initial_covariance (float, optional): The initial uncertainty in the state estimate (P). Defaults to 1000.0.\n",
        "        process_noise (float, optional): The covariance of the process noise (Q). Defaults to 0.1.\n",
        "        measurement_noise (float, optional): The covariance of the measurement noise (R). Defaults to 1.0.\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        np.array: A 1D numpy array of smoothed values.\n",
        "    \"\"\"\n",
        "    n_measurements = len(measurements)\n",
        "    smoothed_values = np.zeros(n_measurements)\n",
        "\n",
        "    # Initialize state estimate and covariance\n",
        "    # State [x] - the estimated true value\n",
        "    # Covariance [P] - the uncertainty in the state estimate\n",
        "    x_hat = 0.0  # Initial state estimate\n",
        "    P = initial_covariance # Initial covariance (high uncertainty)\n",
        "\n",
        "    # Define system parameters (simplified for a static system with noise)\n",
        "    # State transition matrix [A] - assumes the state doesn't change over time\n",
        "    A = 1.0\n",
        "    # Control input matrix [B] - no control input\n",
        "    B = 0.0\n",
        "    # Measurement matrix [H] - relates the state to the measurement\n",
        "    H = 1.0\n",
        "    # Process noise covariance [Q] - uncertainty in the system model\n",
        "    Q = process_noise # Small value assuming the true value is relatively stable\n",
        "    # Measurement noise covariance [R] - uncertainty in the measurements\n",
        "    R = measurement_noise # Adjust based on expected measurement noise\n",
        "\n",
        "    for k in range(n_measurements):\n",
        "        # Prediction Step\n",
        "        # Predicted state estimate\n",
        "        x_hat_minus = A * x_hat + B * 0 # No control input (u=0)\n",
        "        # Predicted covariance\n",
        "        P_minus = A * P * A + Q\n",
        "\n",
        "        # Update Step\n",
        "        # Kalman Gain\n",
        "        K = P_minus * H / (H * P_minus * H + R)\n",
        "\n",
        "        # Updated state estimate\n",
        "        x_hat = x_hat_minus + K * (measurements[k] - H * x_hat_minus)\n",
        "\n",
        "        # Updated covariance\n",
        "        P = (1 - K * H) * P_minus\n",
        "\n",
        "        # Store the smoothed value\n",
        "        smoothed_values[k] = x_hat\n",
        "\n",
        "    return smoothed_values"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Measurement and Storing Smooth values"
      ],
      "metadata": {
        "id": "mNe0hHZXIB4Y"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "d05b86fc",
        "outputId": "060ef2fa-b858-4923-9070-c769c3ac3714"
      },
      "source": [
        "cognitive_load_measurements = df_predictions['cognitive_load'].values\n",
        "smoothed_cognitive_load = apply_kalman_filter(cognitive_load_measurements)\n",
        "df_predictions['smoothed_cognitive_load'] = smoothed_cognitive_load\n",
        "display(df_predictions.head())"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   cognitive_load  engagement_level  smoothed_cognitive_load\n",
              "0         75.8690                 4                75.793214\n",
              "1         75.2602                 5                75.514137\n",
              "2         86.8593                 5                79.871568\n",
              "3         80.7670                 5                80.163641\n",
              "4         80.0618                 4                80.133208"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f9318b0c-d3e0-423c-a7df-477d402c8f0b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cognitive_load</th>\n",
              "      <th>engagement_level</th>\n",
              "      <th>smoothed_cognitive_load</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>75.8690</td>\n",
              "      <td>4</td>\n",
              "      <td>75.793214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>75.2602</td>\n",
              "      <td>5</td>\n",
              "      <td>75.514137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>86.8593</td>\n",
              "      <td>5</td>\n",
              "      <td>79.871568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>80.7670</td>\n",
              "      <td>5</td>\n",
              "      <td>80.163641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>80.0618</td>\n",
              "      <td>4</td>\n",
              "      <td>80.133208</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f9318b0c-d3e0-423c-a7df-477d402c8f0b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f9318b0c-d3e0-423c-a7df-477d402c8f0b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f9318b0c-d3e0-423c-a7df-477d402c8f0b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-89eab11d-26b8-4fd3-b368-265371f38254\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-89eab11d-26b8-4fd3-b368-265371f38254')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-89eab11d-26b8-4fd3-b368-265371f38254 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df_predictions\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"cognitive_load\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.6608264801427675,\n        \"min\": 75.2602,\n        \"max\": 86.8593,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          75.2602,\n          80.0618,\n          86.8593\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"engagement_level\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 4,\n        \"max\": 5,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          5,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"smoothed_cognitive_load\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.4160154039359543,\n        \"min\": 75.51413715348997,\n        \"max\": 80.16364073356519,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          75.51413715348997,\n          80.13320799601364\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot Smoothed Results"
      ],
      "metadata": {
        "id": "R4NS0t0nI8wP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "ce77555b",
        "outputId": "91e2c9c7-2e5c-41e1-acac-1ff1adf46bca"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "# Create a Plotly figure object\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add a scatter trace for the original 'cognitive_load' data\n",
        "fig.add_trace(go.Scatter(x=df_predictions.index, y=df_predictions['cognitive_load'],\n",
        "                         mode='lines', name='Original Cognitive Load'))\n",
        "\n",
        "# Add a scatter trace for the 'smoothed_cognitive_load' data\n",
        "fig.add_trace(go.Scatter(x=df_predictions.index, y=df_predictions['smoothed_cognitive_load'],\n",
        "                         mode='lines', name='Smoothed Cognitive Load'))\n",
        "\n",
        "# Update the layout with title and axis labels\n",
        "fig.update_layout(\n",
        "    title='Original vs. Smoothed Cognitive Load',\n",
        "    xaxis_title='Row Number',\n",
        "    yaxis_title='Cognitive Load',\n",
        "    legend=dict(\n",
        "        orientation=\"h\",\n",
        "        yanchor=\"bottom\",\n",
        "        y=1.02,\n",
        "        xanchor=\"right\",\n",
        "        x=1\n",
        "    )\n",
        ")\n",
        "\n",
        "# Display the Plotly figure\n",
        "fig.show()"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"40660f69-3a8e-4f7a-b7f9-f07fce051c8b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"40660f69-3a8e-4f7a-b7f9-f07fce051c8b\")) {                    Plotly.newPlot(                        \"40660f69-3a8e-4f7a-b7f9-f07fce051c8b\",                        [{\"mode\":\"lines\",\"name\":\"Original Cognitive Load\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99],\"y\":[75.869,75.2602,86.8593,80.767,80.0618,71.347,88.0693,81.5807,82.1533,89.1997,95.3333,100.0,84.8895,93.3333,93.104,89.618,86.794,75.6203,90.2763,78.1565,87.3333,82.306,81.2125,82.6207,75.72,85.783,75.339,76.4922,84.2548,78.7903,78.6667,66.2507,76.5057,76.5172,91.9173,82.1612,95.8282,98.2683,99.331,91.3972,98.8702,93.9178,100.0,90.5048,92.0442,91.3203,88.5202,99.0107,94.3583,92.5117,97.3647,86.5985,91.6882,100.0,90.988,72.9095,83.1815,85.8997,72.5815,79.8448,78.6667,62.994,73.0757,62.1028,72.0,75.3643,77.8432,75.187,83.5122,82.0,78.9677,87.2215,75.8163,85.3333,86.3698,83.3195,99.9907,94.0,80.2258,100.0,90.7135,91.5147,81.6462,74.4538,77.0672,75.5395,78.0,60.3732,55.6593,49.4583,64.6667,57.0098,60.5355,54.4432,46.067,50.525,49.0383,72.6667,68.8353,67.4218],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Smoothed Cognitive Load\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99],\"y\":[75.79321436419939,75.51413715348997,79.87156762614342,80.16364073356519,80.13320799601364,77.6281265691743,80.53117360392994,80.81909103117259,81.18249372956357,83.35784403267772,86.60060216813503,90.22500928002104,88.78263903790756,90.01245957635622,90.84781446050732,90.51553938256104,89.51008910847595,85.75757035150335,86.97835159569082,84.5950544940133,85.33481168385924,84.51655753291848,83.62394478523798,83.3529117996719,81.29083255388726,82.50441971631037,80.56863688058198,79.46736207780671,80.76071820720983,80.22839748070439,79.80649520191676,76.1443129059499,76.24194387451519,76.31630602675884,80.53101146266492,80.97141702269616,84.9850692331632,88.57361653889136,91.47979050454295,91.45747816669845,93.46007101687032,93.58372934501745,95.3171247195178,94.01704530299378,93.4840688893276,92.89951328282079,91.71641459573436,93.68701110886173,93.86836397276318,93.50185277309873,94.54542494695728,92.39851380725902,92.2066181198498,94.31204864626028,93.41403625590954,87.87460841482826,86.60673602357532,86.41572584978942,82.67832380003357,81.91282974396836,81.035867629094,76.16174501532835,75.32803078432188,71.75515253450125,71.82129959826676,72.77846316545872,74.14673328280516,74.42776779845866,76.88198358903183,78.2646475149159,78.45458151103315,80.8230189997999,79.47042276090818,81.0543154664897,82.49032663232515,82.71433296832105,87.38165084050357,89.16963897827699,86.75340532031574,90.3320551591783,90.43510485241266,90.72676418783986,88.27359336519575,84.5400903408056,82.52124259460138,80.63508146298098,79.92319783696885,74.64164447923454,69.51344620228195,64.0954238757415,64.24975766940389,62.29383813132669,61.818812162578006,59.82624472050152,56.109099288595935,54.60052017807372,53.097851845169316,58.384497733369756,61.20784688474216,62.88658491910821],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"legend\":{\"orientation\":\"h\",\"yanchor\":\"bottom\",\"y\":1.02,\"xanchor\":\"right\",\"x\":1},\"title\":{\"text\":\"Original vs. Smoothed Cognitive Load\"},\"xaxis\":{\"title\":{\"text\":\"Row Number\"}},\"yaxis\":{\"title\":{\"text\":\"Cognitive Load\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('40660f69-3a8e-4f7a-b7f9-f07fce051c8b');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simmon's Rule Integration\n",
        "\n",
        "This section utilizes Simpson's Rule, a method for numerical integration, to calculate the approximate definite integral of the smoothed cognitive load values over a sequence of data points.\n",
        "\n",
        "*   **Purpose**: To quantify the cumulative \"cognitive effort\" or \"cognitive load area under the curve\" over a specific period or sequence of tasks. This integral value provides a single metric summarizing the cognitive load trend.\n",
        "*   **Simpson's Rule**: A numerical technique that approximates the integral of a function by dividing the area under the curve into segments and using parabolic segments to approximate the curve. It requires an odd number of data points (after potentially dropping one if the input is even).\n",
        "*   **Discretization**: The calculated integral value is then discretized into a fixed number of buckets (e.g., 5 levels). This converts the continuous integral value into a categorical level (`simpsons_integral_level`) which is used as one of the state variables for the Q-Learning decision engine. This discretization simplifies the state space for the reinforcement learning agent."
      ],
      "metadata": {
        "id": "lus3BNyzJ1oU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5da90596"
      },
      "source": [
        "### Simpson's Rule Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67a24e9d"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def simpsons_rule(y, h):\n",
        "    \"\"\"\n",
        "    Applies Simpson's Rule for numerical integration.\n",
        "\n",
        "    Args:\n",
        "        y (np.array): A 1D numpy array of function values (the data points).\n",
        "        h (float): The step size (the distance between consecutive data points).\n",
        "\n",
        "    Returns:\n",
        "        float: The approximate value of the integral.\n",
        "        None: If an error occurs (e.g., less than 3 points after dropping).\n",
        "    \"\"\"\n",
        "    n = len(y)\n",
        "\n",
        "    # Check if the number of data points is even and drop the first point if necessary\n",
        "    if n % 2 == 0:\n",
        "        print(\"Number of data points is even. Dropping the first data point to apply Simpson's Rule.\")\n",
        "        y = y[1:]\n",
        "        n = len(y) # Update n after dropping the point\n",
        "\n",
        "    if n < 3:\n",
        "        print(\"Error: Simpson's Rule requires at least 3 points (after potential dropping).\")\n",
        "        return None\n",
        "\n",
        "    integral = y[0] + y[n-1]\n",
        "    for i in range(1, n - 1, 2):\n",
        "        integral += 4 * y[i]\n",
        "    for i in range(2, n - 2, 2):\n",
        "        integral += 2 * y[i]\n",
        "\n",
        "    integral = integral * h / 3\n",
        "    return integral"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simmon's Rule Tester"
      ],
      "metadata": {
        "id": "LHH2Aq71U_KU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assuming the data points are equally spaced by 1 (since they are row numbers)\n",
        "# If the data represents a time series with a different time step, 'h' should be that time step.\n",
        "h = 3\n",
        "cognitive_load_values = df_predictions['smoothed_cognitive_load'].values\n",
        "\n",
        "\n",
        "# Apply Simpson's Rule\n",
        "simpsons_integral = simpsons_rule(cognitive_load_values, h)\n",
        "\n",
        "if simpsons_integral is not None:\n",
        "    print(f\"Approximate integral of smoothed cognitive load using Simpson's Rule: {simpsons_integral}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXvgKLURU9_B",
        "outputId": "8a995c9f-2724-41e1-9f04-644a8b044b63"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of data points is even. Dropping the first data point to apply Simpson's Rule.\n",
            "Approximate integral of smoothed cognitive load using Simpson's Rule: 24016.36171630266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discretize Simpson's Rule into Buckets"
      ],
      "metadata": {
        "id": "3UG_6FmqqVAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np # Ensure numpy is imported\n",
        "\n",
        "def discretize_simpsons_result(simpsons_integral_value, num_buckets=5, historical_integral_values=None):\n",
        "    \"\"\"\n",
        "    Discretizes the result of Simpson's Rule into a specified number of buckets\n",
        "    and returns the bucket number as an integer.\n",
        "\n",
        "    Args:\n",
        "        simpsons_integral_value (float): The result from the simpsons_rule function.\n",
        "        num_buckets (int, optional): The number of buckets to discretize into.\n",
        "                                     Defaults to 5. Must be at least 2.\n",
        "        historical_integral_values (list or np.array, optional): A collection of\n",
        "                                   historically observed integral values. If provided,\n",
        "                                   the discretization range will be based on the min/max\n",
        "                                   of these values. If None, a dynamic range based on\n",
        "                                   the input value and estimated max is used (less robust).\n",
        "\n",
        "    Returns:\n",
        "        int: The bucket number (1-based index).\n",
        "             Returns None if simpsons_integral_value is None or num_buckets < 2.\n",
        "    \"\"\"\n",
        "    if simpsons_integral_value is None or num_buckets < 2:\n",
        "        print(\"Error: Invalid input for discretization.\")\n",
        "        return None\n",
        "\n",
        "    # --- IMPROVEMENT: Use a more robust method for defining the discretization range ---\n",
        "    # Option A: Use historical data to define the range\n",
        "    if historical_integral_values is not None and len(historical_integral_values) > 1:\n",
        "        min_range = np.min(historical_integral_values)\n",
        "        max_range = np.max(historical_integral_values)\n",
        "        print(f\"Using historical data range for discretization: [{min_range:.2f}, {max_range:.2f}]\")\n",
        "    # Option B: Define a fixed range based on domain knowledge or prior analysis\n",
        "    # elif some_fixed_min is not None and some_fixed_max is not None:\n",
        "    #    min_range = some_fixed_min\n",
        "    #    max_range = some_fixed_max\n",
        "    #    print(f\"Using fixed range for discretization: [{min_range:.2f}, {max_range:.2f}]\")\n",
        "    # Option C (Fallback - original less robust dynamic range):\n",
        "    else:\n",
        "        # Use the range of 'smoothed_cognitive_load' as a hint for the integral's scale:\n",
        "        # Assuming non-negative cognitive load, min possible integral is 0.\n",
        "        min_range = 0\n",
        "\n",
        "        # Attempt to use the global variable cognitive_load_values if available\n",
        "        # This is not ideal as it relies on a global variable, but matches the original code's potential context\n",
        "        global cognitive_load_values # Declare intent to use global variable\n",
        "        if 'cognitive_load_values' in globals() and cognitive_load_values is not None and len(cognitive_load_values) > 0:\n",
        "             # A rough estimate of max integral: max_smoothed_load * number of steps * h\n",
        "             # Need the step size 'h' as well. If 'h' is a global, try to use it.\n",
        "             global h # Declare intent to use global variable\n",
        "             if 'h' in globals() and h is not None:\n",
        "                 max_range_estimate = np.max(cognitive_load_values) * len(cognitive_load_values) * h\n",
        "                 # If simpsons_integral_value exceeds this rough max, adjust the max\n",
        "                 max_range = max_range_estimate * 1.1 if simpsons_integral_value > max_range_estimate else max_range_estimate\n",
        "                 print(f\"Using dynamic range based on global data/h for discretization: [{min_range:.2f}, {max_range:.2f}]\")\n",
        "             else:\n",
        "                 # Fallback if h is not available globally\n",
        "                 max_range = simpsons_integral_value * 2 if simpsons_integral_value > 0 else 100 # Ensure a positive max\n",
        "                 print(f\"Using dynamic range based on input value for discretization: [{min_range:.2f}, {max_range:.2f}]\")\n",
        "        else:\n",
        "             # Fallback if cognitive_load_values is not available globally\n",
        "             max_range = simpsons_integral_value * 2 if simpsons_integral_value > 0 else 100 # Ensure a positive max\n",
        "             print(f\"Using dynamic range based on input value for discretization: [{min_range:.2f}, {max_range:.2f}]\")\n",
        "\n",
        "\n",
        "    # Ensure min is less than max for bucket creation\n",
        "    if min_range >= max_range:\n",
        "        # Fallback if calculated range is invalid\n",
        "        print(f\"Warning: Calculated range [{min_range:.2f}, {max_range:.2f}] is invalid. Using a default range based on input value.\")\n",
        "        min_range = 0 # Assuming non-negative\n",
        "        max_range = simpsons_integral_value + 1 # Use the value itself plus a small buffer\n",
        "        if max_range <= min_range: # Ensure max is strictly greater than min\n",
        "            max_range = min_range + 1\n",
        "\n",
        "\n",
        "    # Define the bucket edges\n",
        "    bins = np.linspace(min_range, max_range, num_buckets + 1)\n",
        "\n",
        "    # Find which bucket the integral value falls into\n",
        "    # Use pd.cut to assign the value to a bin\n",
        "    # We need to put the single value into a Series or DataFrame to use pd.cut\n",
        "    integral_series = pd.Series([simpsons_integral_value])\n",
        "    # labels argument can be False or None to just return the bin index\n",
        "    # right=True means intervals are like (a, b], include_lowest=True handles the lowest bound.\n",
        "    # If value == max_range, it falls into the last bin.\n",
        "    bucket_index_category = pd.cut(integral_series, bins=bins, include_lowest=True, labels=False)\n",
        "\n",
        "    # pd.cut returns a categorical type, get the index\n",
        "    # Handle the case where the value is exactly at the edge or outside the range if include_lowest is False\n",
        "    # With include_lowest=True, the first bin is inclusive of the lower bound.\n",
        "    # If the value is outside the range, pd.cut might return NaN.\n",
        "    if bucket_index_category.isnull().any():\n",
        "        # This should ideally not happen with a well-defined range, but as a safeguard:\n",
        "        print(f\"Warning: Simpson's integral value {simpsons_integral_value} is outside the calculated range [{min_range:.2f}, {max_range:.2f}]. Assigning to the closest bucket.\")\n",
        "        # Assign to the closest bucket - either the first or the last\n",
        "        if simpsons_integral_value < min_range:\n",
        "            bucket_index = 0\n",
        "        else: # Value is greater than max_range\n",
        "             bucket_index = num_buckets - 1\n",
        "    else:\n",
        "        # Get the integer index of the bucket (0-based)\n",
        "        bucket_index = bucket_index_category[0]\n",
        "\n",
        "        # Ensure the index is within valid bounds [0, num_buckets - 1]\n",
        "        bucket_index = int(np.clip(bucket_index, 0, num_buckets - 1))\n",
        "\n",
        "\n",
        "    # Return the bucket number (1-based index)\n",
        "    return bucket_index + 1"
      ],
      "metadata": {
        "id": "BvWgeM1lq5Yb"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discretization Tester"
      ],
      "metadata": {
        "id": "E_s-ZWjAu5q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if simpsons_integral is not None:\n",
        "    # Discretize the integral into 5 buckets\n",
        "    num_buckets = 5 # Adjustable number of buckets\n",
        "    bucket_number = discretize_simpsons_result(simpsons_integral, num_buckets=num_buckets)\n",
        "\n",
        "    if bucket_number is not None:\n",
        "        print(f\"Simpson's Integral: {simpsons_integral}\")\n",
        "        print(f\"Discretized Bucket Number: {bucket_number}\")\n",
        "\n",
        "    # Example with a different number of buckets\n",
        "    num_buckets_alt = 7\n",
        "    bucket_number_alt = discretize_simpsons_result(simpsons_integral, num_buckets=num_buckets_alt)\n",
        "\n",
        "    if bucket_number_alt is not None:\n",
        "        print(f\"\\nDiscretized into {num_buckets_alt} buckets:\")\n",
        "        print(f\"Discretized Bucket Number: {bucket_number_alt}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91bYCdmbu2gN",
        "outputId": "01feca5a-4139-4f1e-ab0b-391a45fd5b8e"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using dynamic range based on global data/h for discretization: [0.00, 28595.14]\n",
            "Simpson's Integral: 24016.36171630266\n",
            "Discretized Bucket Number: 5\n",
            "Using dynamic range based on global data/h for discretization: [0.00, 28595.14]\n",
            "\n",
            "Discretized into 7 buckets:\n",
            "Discretized Bucket Number: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-Learning Decision Engine ver. 3\n",
        "\n",
        "A reinforcement learning decision engine to recommend the next task in an adaptive learning system. The system should have a state defined by a tuple of `simpsons_integral_level` (int: 1 to 5), `engagement_level` (int: 1 to 5), `task_completed` (0 or 1), and `prev_task_type` (str: A, B, C, or D). The action space is a tuple of `(task_type, difficulty)`, where `task_type` rotates A  B  C  D  A and `difficulty` is an integer from 0 to 10. The reward function should encourage balanced cognitive load (mid-range `simpsons_integral_level`), higher `engagement_level`, and completed tasks. The implementation should include Q-table initialization, epsilon-greedy action selection, reward logic, Q-table updating, and a simulation of a few learning episodes. Finally, print the Q-table or visualize top recommended actions per state. Avoid external RL libraries."
      ],
      "metadata": {
        "id": "n7ybYZjNllag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Engine Functions"
      ],
      "metadata": {
        "id": "PlMA-pWltrFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Environment Setup Functions ---\n",
        "\n",
        "def define_state_space():\n",
        "    \"\"\"\n",
        "    Defines the state space for the Q-learning agent in the adaptive learning environment.\n",
        "\n",
        "    The state is a tuple comprising:\n",
        "    - simpsons_integral_level (int: 1-5): Discretized cognitive load trend.\n",
        "    - engagement_level (int: 1-5): Learner's engagement level.\n",
        "    - task_completed (int: 0 or 1): Status of the previous task.\n",
        "    - prev_task_type (str: 'A', 'B', 'C', 'D'): Type of the previously presented task.\n",
        "\n",
        "    This function generates all possible combinations of these state variables\n",
        "    and creates mappings between state tuples and unique integer indices.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - all_states_tuple (list): List of all unique possible state tuples.\n",
        "            - state_to_index (dict): Dictionary mapping each state tuple to its unique integer index.\n",
        "            - index_to_state (dict): Dictionary mapping each integer index back to its corresponding state tuple.\n",
        "            - num_states (int): The total count of unique states in the state space.\n",
        "    \"\"\"\n",
        "    # Define the possible values for each component of the state tuple\n",
        "    simpsons_integral_levels = range(1, 6)\n",
        "    engagement_levels = range(1, 6)\n",
        "    task_completed_statuses = [0, 1]\n",
        "    prev_task_types = ['A', 'B', 'C', 'D']\n",
        "\n",
        "    # Generate all combinations using itertools.product\n",
        "    all_states_tuple = list(itertools.product(\n",
        "        simpsons_integral_levels,\n",
        "        engagement_levels,\n",
        "        task_completed_statuses,\n",
        "        prev_task_types\n",
        "    ))\n",
        "\n",
        "    # Create mappings for efficient lookup\n",
        "    state_to_index = {state: index for index, state in enumerate(all_states_tuple)}\n",
        "    index_to_state = {index: state for state, index in state_to_index.items()}\n",
        "\n",
        "    num_states = len(all_states_tuple)\n",
        "\n",
        "    return all_states_tuple, state_to_index, index_to_state, num_states\n",
        "\n",
        "\n",
        "def define_action_space():\n",
        "    \"\"\"\n",
        "    Defines the action space for the Q-learning agent.\n",
        "\n",
        "    An action is a tuple representing the next task to present:\n",
        "    - task_type (str: 'A', 'B', 'C', 'D'): The category of the task.\n",
        "    - difficulty (int: 0-10): The difficulty level of the task.\n",
        "\n",
        "    This function generates all possible combinations of task type and difficulty\n",
        "    and creates mappings between action tuples and unique integer indices.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - all_actions_tuple (list): List of all unique possible action tuples.\n",
        "            - action_to_index (dict): Dictionary mapping each action tuple to its unique integer index.\n",
        "            - index_to_action (dict): Dictionary mapping each integer index back to its corresponding action tuple.\n",
        "            - num_actions (int): The total count of unique actions in the action space.\n",
        "    \"\"\"\n",
        "    # Define the possible values for each component of the action tuple\n",
        "    task_types = ['A', 'B', 'C', 'D']\n",
        "    difficulties = range(0, 11) # 0 to 10 inclusive\n",
        "\n",
        "    # Generate all combinations using itertools.product\n",
        "    all_actions_tuple = list(itertools.product(task_types, difficulties))\n",
        "\n",
        "    # Create mappings for efficient lookup\n",
        "    action_to_index = {action: index for index, action in enumerate(all_actions_tuple)}\n",
        "    index_to_action = {index: action for action, index in action_to_index.items()}\n",
        "\n",
        "    num_actions = len(all_actions_tuple)\n",
        "\n",
        "    return all_actions_tuple, action_to_index, index_to_action, num_actions\n",
        "\n",
        "\n",
        "def initialize_q_table(num_states: int, num_actions: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Initializes the Q-table with zeros.\n",
        "\n",
        "    The Q-table is a fundamental component of Q-learning, storing the learned\n",
        "    value for taking a specific action in a specific state.\n",
        "\n",
        "    Args:\n",
        "        num_states (int): The total number of states in the environment.\n",
        "        num_actions (int): The total number of actions available to the agent.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The initialized Q-table as a NumPy array of shape (num_states, num_actions),\n",
        "                    with all values set to 0.0 initially.\n",
        "    \"\"\"\n",
        "    # Create a NumPy array filled with zeros with dimensions corresponding to the state and action space sizes\n",
        "    q_table = np.zeros((num_states, num_actions))\n",
        "    return q_table\n",
        "\n",
        "\n",
        "# --- Validation Functions ---\n",
        "\n",
        "def is_valid_state(state_tuple: tuple, state_to_index: dict) -> bool:\n",
        "    \"\"\"\n",
        "    Checks if a given state tuple is a valid state within the defined state space.\n",
        "\n",
        "    Validity is determined by whether the state tuple exists as a key in the\n",
        "    provided state-to-index mapping.\n",
        "\n",
        "    Args:\n",
        "        state_tuple (tuple): The state tuple to validate.\n",
        "        state_to_index (dict): Dictionary mapping state tuples to their indices,\n",
        "                               representing the set of valid states.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the state is valid, False otherwise. Prints an error message if invalid.\n",
        "    \"\"\"\n",
        "    if state_tuple in state_to_index:\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"Validation Error: Invalid state tuple {state_tuple} not found in state space mapping.\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def is_valid_action(action_tuple: tuple, action_to_index: dict) -> bool:\n",
        "    \"\"\"\n",
        "    Checks if a given action tuple is a valid action within the defined action space.\n",
        "\n",
        "    Validity is determined by whether the action tuple exists as a key in the\n",
        "    provided action-to-index mapping.\n",
        "\n",
        "    Args:\n",
        "        action_tuple (tuple): The action tuple to validate.\n",
        "        action_to_index (dict): Dictionary mapping action tuples to their indices,\n",
        "                                representing the set of valid actions.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the action is valid, False otherwise. Prints an error message if invalid.\n",
        "    \"\"\"\n",
        "    if action_tuple in action_to_index:\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"Validation Error: Invalid action tuple {action_tuple} not found in action space mapping.\")\n",
        "        return False\n",
        "\n",
        "# --- Simulation Environment Dynamics (Modularized) ---\n",
        "\n",
        "def simulate_cognitive_load_change(current_simpsons_level: int, chosen_difficulty: int, current_completed: int) -> int:\n",
        "    \"\"\"\n",
        "    Simulates the change in the simpsons_integral_level (cognitive load)\n",
        "    based on the chosen task's difficulty and the outcome of the previous task.\n",
        "\n",
        "    Args:\n",
        "        current_simpsons_level (int): The learner's current discretized cognitive load level (1-5).\n",
        "        chosen_difficulty (int): The difficulty level of the task chosen by the agent (0-10).\n",
        "        current_completed (int): Whether the previous task was completed (0 or 1).\n",
        "\n",
        "    Returns:\n",
        "        int: The simulated next discretized cognitive load level (1-5), clamped to the valid range.\n",
        "    \"\"\"\n",
        "    # Heuristic: Difficulty influences load change. Difficulty 5 is neutral.\n",
        "    # Higher difficulty increases load, lower difficulty decreases load.\n",
        "    simpsons_change = (chosen_difficulty - 5) * 0.2\n",
        "    # Heuristic: Completing a task might slightly reduce load in the subsequent state.\n",
        "    if current_completed == 1:\n",
        "        simpsons_change -= 0.5\n",
        "    # Calculate potential next level and clamp to the valid range [1, 5]\n",
        "    next_simpsons_level_float = current_simpsons_level + simpsons_change\n",
        "    return int(np.clip(round(next_simpsons_level_float), 1, 5))\n",
        "\n",
        "def simulate_engagement_change(current_engagement: int, chosen_difficulty: int, current_completed: int) -> int:\n",
        "    \"\"\"\n",
        "    Simulates the change in the engagement_level based on the chosen task's\n",
        "    difficulty and the outcome of the previous task.\n",
        "\n",
        "    Args:\n",
        "        current_engagement (int): The learner's current engagement level (1-5).\n",
        "        chosen_difficulty (int): The difficulty level of the task chosen by the agent (0-10).\n",
        "        current_completed (int): Whether the previous task was completed (0 or 1).\n",
        "\n",
        "    Returns:\n",
        "        int: The simulated next engagement level (1-5), clamped to the valid range.\n",
        "    \"\"\"\n",
        "    engagement_change = 0\n",
        "\n",
        "    # Heuristics: Task difficulty and completion influence engagement.\n",
        "    # Challenging completed tasks and mid-range difficulty are generally positive.\n",
        "    # Very easy or very difficult tasks (especially if not completed) might be negative.\n",
        "    if chosen_difficulty > 7 and current_completed == 1:\n",
        "        engagement_change += 1 # Challenging and completed: boosts engagement\n",
        "    elif chosen_difficulty < 3 and current_completed == 0:\n",
        "        engagement_change -= 1 # Too easy and not completed (boredom?): harms engagement\n",
        "    elif 3 <= chosen_difficulty <= 7:\n",
        "         engagement_change += 0.5 # Mid-range difficulty is somewhat engaging\n",
        "\n",
        "    # Heuristic: Tendency towards average engagement level (3)\n",
        "    engagement_change += (current_engagement - 3) * 0.1\n",
        "\n",
        "    # Calculate potential next level and clamp to the valid range [1, 5]\n",
        "    next_engagement_level_float = current_engagement + engagement_change\n",
        "    return int(np.clip(round(next_engagement_level_float), 1, 5))\n",
        "\n",
        "def simulate_task_completion(next_engagement_level: int, chosen_difficulty: int) -> int:\n",
        "    \"\"\"\n",
        "    Simulates the completion status (0 or 1) for the *next* task,\n",
        "    based on the simulated engagement level for that task and its chosen difficulty.\n",
        "\n",
        "    Args:\n",
        "        next_engagement_level (int): The simulated engagement level for the next task (1-5).\n",
        "        chosen_difficulty (int): The difficulty level of the task chosen by the agent (0-10).\n",
        "\n",
        "    Returns:\n",
        "        int: The simulated completion status (0: not completed, 1: completed) for the next task.\n",
        "    \"\"\"\n",
        "    # Use a sigmoid-like probability based on engagement and difficulty.\n",
        "    # Higher engagement and lower difficulty increase the probability of completion.\n",
        "    completion_probability = 1.0 / (1 + np.exp(-(next_engagement_level * 0.5 - chosen_difficulty * 0.2)))\n",
        "    # Randomly determine completion based on the calculated probability\n",
        "    return 1 if random.random() < completion_probability else 0\n",
        "\n",
        "def simulate_next_state_and_reward(current_state: tuple, action: tuple, state_to_index: dict, action_to_index: dict) -> tuple[tuple | None, float | None]:\n",
        "    \"\"\"\n",
        "    Simulates the environment's response to an action taken in a given state.\n",
        "\n",
        "    This function acts as the environment's dynamics model. It determines the\n",
        "    subsequent state and the immediate reward received. It relies on modular\n",
        "    simulation functions for state component changes and the calculate_reward\n",
        "    function for reward determination. Includes input validation.\n",
        "\n",
        "    Args:\n",
        "        current_state (tuple): The state tuple before the action was taken.\n",
        "        action (tuple): The action tuple chosen by the agent.\n",
        "        state_to_index (dict): Mapping from state tuple to index (for validation).\n",
        "        action_to_index (dict): Mapping from action tuple to index (for validation).\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing (next_state, reward).\n",
        "               - next_state (tuple): The simulated state after taking the action.\n",
        "               - reward (float): The immediate reward obtained.\n",
        "               Returns (None, None) if input validation fails or if the simulated\n",
        "               next state is invalid for reward calculation.\n",
        "    \"\"\"\n",
        "    # Validate input state and action\n",
        "    if not is_valid_state(current_state, state_to_index) or not is_valid_action(action, action_to_index):\n",
        "         return None, None # Error messages printed by validation functions\n",
        "\n",
        "    # Unpack current state and action\n",
        "    current_simpsons_level, current_engagement, current_completed, current_prev_type = current_state\n",
        "    chosen_task_type, chosen_difficulty = action\n",
        "\n",
        "    # Simulate components of the next state using modular functions\n",
        "    next_simpsons_level = simulate_cognitive_load_change(current_simpsons_level, chosen_difficulty, current_completed)\n",
        "    next_engagement_level = simulate_engagement_change(current_engagement, chosen_difficulty, current_completed)\n",
        "    # Simulate completion for the *next* task based on its characteristics and predicted engagement\n",
        "    next_task_completed = simulate_task_completion(next_engagement_level, chosen_difficulty)\n",
        "\n",
        "    # The previous task type in the next state becomes the task type of the action just taken\n",
        "    next_prev_task_type = chosen_task_type\n",
        "\n",
        "    # Construct the simulated next state tuple\n",
        "    next_state_simulated = (next_simpsons_level, next_engagement_level, next_task_completed, next_prev_task_type)\n",
        "\n",
        "    # Calculate the immediate reward based on the simulated next state\n",
        "    # calculate_reward internally validates the next_state\n",
        "    reward_simulated = calculate_reward(current_state, next_state_simulated, state_to_index)\n",
        "\n",
        "    return next_state_simulated, reward_simulated\n",
        "\n",
        "# --- Task type Sequencing ---\n",
        "\n",
        "def get_next_task_type_in_sequence(prev_task_type: str) -> str:\n",
        "    \"\"\"\n",
        "    Determines the next task type in the sequence A -> B -> C -> D -> A.\n",
        "\n",
        "    This function implements a strict rotation of task types.\n",
        "\n",
        "    Args:\n",
        "        prev_task_type (str): The type of the previous task ('A', 'B', 'C', or 'D').\n",
        "                              It is assumed this input is always one of these four values\n",
        "                              based on the state space definition.\n",
        "\n",
        "    Returns:\n",
        "        str: The next task type in the A->B->C->D->A sequence. Returns 'A' as a default\n",
        "             for any unexpected input, though this case should ideally not occur\n",
        "             with proper state handling.\n",
        "    \"\"\"\n",
        "    if prev_task_type == 'A':\n",
        "        return 'B'\n",
        "    elif prev_task_type == 'B':\n",
        "        return 'C'\n",
        "    elif prev_task_type == 'C':\n",
        "        return 'D'\n",
        "    elif prev_task_type == 'D':\n",
        "        return 'A'\n",
        "    else:\n",
        "        # This else block serves as a safeguard for unexpected input,\n",
        "        # although the state space is defined to only include A, B, C, D.\n",
        "        print(f\"Warning: Invalid previous task type '{prev_task_type}' provided to get_next_task_type_in_sequence. Defaulting to 'A'.\")\n",
        "        return 'A'\n",
        "\n",
        "# --- Core Q-Learning Functions ---\n",
        "\n",
        "def epsilon_greedy_action_selection(current_state: tuple, q_table: np.ndarray, state_to_index: dict, index_to_action: dict, epsilon: float) -> tuple | None:\n",
        "    \"\"\"\n",
        "    Selects an action using the epsilon-greedy policy, strictly enforcing the\n",
        "    task type rotation (A->B->C->D->A).\n",
        "\n",
        "    This policy balances exploration (trying random actions) and exploitation\n",
        "    (choosing the action with the highest learned Q-value) only among actions\n",
        "    that match the required next task type based on the current state's\n",
        "    previous task type.\n",
        "\n",
        "    Args:\n",
        "        current_state (tuple): The agent's current state tuple.\n",
        "        q_table (np.ndarray): The learned Q-table.\n",
        "        state_to_index (dict): Mapping from state tuple to index (for state lookup).\n",
        "        index_to_action (dict): Mapping from index to action tuple (for action lookup).\n",
        "        epsilon (float): The probability of choosing a random action (exploration rate, 0 to 1).\n",
        "\n",
        "    Returns:\n",
        "        tuple: The selected action tuple (task_type, difficulty).\n",
        "        None: If the current state is invalid (validated internally) or no valid\n",
        "              actions are found for the required task type.\n",
        "    \"\"\"\n",
        "    # Validate the current state\n",
        "    if not is_valid_state(current_state, state_to_index):\n",
        "        return None # Error message printed by is_valid_state\n",
        "\n",
        "    # Get the index corresponding to the current state\n",
        "    state_index = state_to_index[current_state]\n",
        "\n",
        "    # Determine the required next task type based on the current state's previous task type\n",
        "    # The previous task type is the 4th element (index 3) of the state tuple\n",
        "    prev_task_type = current_state[3]\n",
        "    required_task_type = get_next_task_type_in_sequence(prev_task_type)\n",
        "\n",
        "    # Filter actions to include only those with the required task type\n",
        "    # We need the original indices of these valid actions to access the Q-table\n",
        "    valid_action_indices = [\n",
        "        action_to_index[action] for action in action_to_index\n",
        "        if action[0] == required_task_type # Check the task type component of the action tuple\n",
        "    ]\n",
        "\n",
        "    if not valid_action_indices:\n",
        "        print(f\"Error: No valid actions found for the required task type '{required_task_type}'.\")\n",
        "        return None # Should not happen with the defined action space, but good for safety\n",
        "\n",
        "    # Get the Q-values for the current state, but only for the valid actions\n",
        "    valid_q_values = q_table[state_index, valid_action_indices]\n",
        "\n",
        "    # Decide between exploration and exploitation among the valid actions\n",
        "    if random.random() < epsilon:\n",
        "        # Exploration: Choose a random index from the list of valid action indices\n",
        "        selected_valid_action_index_in_list = random.randrange(len(valid_action_indices))\n",
        "        # Get the original index from the filtered list\n",
        "        selected_original_action_index = valid_action_indices[selected_valid_action_index_in_list]\n",
        "    else:\n",
        "        # Exploitation: Choose the index within the valid_q_values array\n",
        "        # that corresponds to the max Q-value among valid actions\n",
        "        selected_valid_action_index_in_list = np.argmax(valid_q_values)\n",
        "        # Get the original index from the filtered list\n",
        "        selected_original_action_index = valid_action_indices[selected_valid_action_index_in_list]\n",
        "\n",
        "\n",
        "    # Return the corresponding action tuple using the original index\n",
        "    return index_to_action[selected_original_action_index]\n",
        "\n",
        "def calculate_reward(current_state: tuple, next_state: tuple, state_to_index: dict) -> float | None:\n",
        "    \"\"\"\n",
        "    Calculates the immediate reward for transitioning from current_state to next_state.\n",
        "\n",
        "    The reward is based on the characteristics of the *next* state, encouraging:\n",
        "    - Balanced cognitive load (mid-range simpsons_integral_level: 2, 3, 4).\n",
        "    - Higher engagement_level (1 is low, 5 is high).\n",
        "    - Completed tasks (task_completed = 1).\n",
        "\n",
        "    Args:\n",
        "        current_state (tuple): The state before the transition. Not used directly in this\n",
        "                                reward calculation logic, but included for context.\n",
        "        next_state (tuple): The state after the transition.\n",
        "        state_to_index (dict): Mapping from state tuple to index (for next_state validation).\n",
        "\n",
        "    Returns:\n",
        "        float: The calculated reward.\n",
        "        None: If the next state is invalid (validated internally).\n",
        "    \"\"\"\n",
        "    # Validate the next state\n",
        "    if not is_valid_state(next_state, state_to_index):\n",
        "        return None # Error message printed by is_valid_state\n",
        "\n",
        "    # Unpack next state components\n",
        "    simpsons_integral_level, engagement_level, task_completed, prev_task_type = next_state\n",
        "\n",
        "    # Reward component for cognitive load: higher for mid-range, penalty for extremes\n",
        "    reward_cognitive_load: float\n",
        "    if simpsons_integral_level in [2, 3, 4]:\n",
        "        reward_cognitive_load = 10.0\n",
        "    else:\n",
        "        reward_cognitive_load = -5.0\n",
        "\n",
        "    # Reward component for engagement level: increasing reward for higher levels\n",
        "    engagement_reward_mapping = {1: -2.0, 2: 0.0, 3: 2.0, 4: 5.0, 5: 10.0}\n",
        "    # Safe lookup because is_valid_state ensures engagement_level is 1-5\n",
        "    reward_engagement: float = engagement_reward_mapping[engagement_level]\n",
        "\n",
        "    # Reward component for task completion: positive for completed, penalty for not\n",
        "    # Safe check because is_valid_state ensures task_completed is 0 or 1\n",
        "    reward_task_completed: float = 15.0 if task_completed == 1 else -3.0\n",
        "\n",
        "    # Combine rewards with weights (adjust weights to prioritize factors)\n",
        "    weight_cognitive_load = 0.4\n",
        "    weight_engagement = 0.4\n",
        "    weight_task_completed = 0.2\n",
        "\n",
        "    total_reward = (weight_cognitive_load * reward_cognitive_load +\n",
        "                    weight_engagement * reward_engagement +\n",
        "                    weight_task_completed * reward_task_completed)\n",
        "\n",
        "    return total_reward\n",
        "\n",
        "\n",
        "def update_q_table(q_table: np.ndarray,\n",
        "                   current_state: tuple,\n",
        "                   action: tuple,\n",
        "                   reward: float,\n",
        "                   next_state: tuple,\n",
        "                   learning_rate: float,\n",
        "                   discount_factor: float,\n",
        "                   state_to_index: dict,\n",
        "                   action_to_index: dict) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Performs a single step of the Q-learning update rule.\n",
        "\n",
        "    This function implements the core Q-learning equation to update the estimated\n",
        "    value of taking the chosen action in the current state.\n",
        "\n",
        "    Args:\n",
        "        q_table (np.ndarray): The Q-table to update.\n",
        "        current_state (tuple): The state before the action.\n",
        "        action (tuple): The action taken.\n",
        "        reward (float): The immediate reward received.\n",
        "        next_state (tuple): The state after the action.\n",
        "        learning_rate (float): The learning rate (alpha, 0 to 1).\n",
        "        discount_factor (float): The discount factor (gamma, 0 to 1).\n",
        "        state_to_index (dict): Mapping for state lookup (for validation and index).\n",
        "        action_to_index (dict): Mapping for action lookup (for validation and index).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The updated Q-table. Returns the original table if current_state or action is invalid.\n",
        "    \"\"\"\n",
        "    # Validate current state and action\n",
        "    if not is_valid_state(current_state, state_to_index) or not is_valid_action(action, action_to_index):\n",
        "        return q_table # Error messages printed by validation functions\n",
        "\n",
        "    # Get indices for Q-table access\n",
        "    current_state_index = state_to_index[current_state]\n",
        "    action_index = action_to_index[action]\n",
        "\n",
        "    # Determine the maximum Q-value for the next state\n",
        "    # If next_state is invalid, assume no future reward\n",
        "    if not is_valid_state(next_state, state_to_index):\n",
        "        print(f\"Warning: Invalid next_state {next_state} encountered during Q-table update. Assuming max_next_q = 0.0.\")\n",
        "        max_next_q = 0.0\n",
        "    else:\n",
        "        # Get index for next state and find max Q-value across all actions from there\n",
        "        next_state_index = state_to_index[next_state]\n",
        "        max_next_q = np.max(q_table[next_state_index, :])\n",
        "\n",
        "    # Get the current Q-value for the state-action pair\n",
        "    current_q_value = q_table[current_state_index, action_index]\n",
        "\n",
        "    # Q-learning update formula: Q(s, a) = Q(s, a) + alpha * [reward + gamma * max(Q(s', a')) - Q(s, a)]\n",
        "    td_target = reward + discount_factor * max_next_q\n",
        "    td_error = td_target - current_q_value\n",
        "    new_q_value = current_q_value + learning_rate * td_error\n",
        "\n",
        "    # Update the Q-table\n",
        "    q_table[current_state_index, action_index] = new_q_value\n",
        "\n",
        "    return q_table\n",
        "\n",
        "# --- Q-table Analysis Functions ---\n",
        "\n",
        "def get_optimal_action_for_state(current_state: tuple, q_table: np.ndarray, state_to_index: dict, index_to_action: dict) -> tuple | None:\n",
        "    \"\"\"\n",
        "    Determines the optimal action for a given state based on the learned Q-table.\n",
        "\n",
        "    The optimal action is the one with the highest Q-value for the specified state.\n",
        "    This represents the agent's greedy policy after learning.\n",
        "\n",
        "    Args:\n",
        "        current_state (tuple): The state tuple for which to find the optimal action.\n",
        "        q_table (np.ndarray): The learned Q-table.\n",
        "        state_to_index (dict): Mapping for state lookup (for validation and index).\n",
        "        index_to_action (dict): Mapping for action lookup (to get action tuple from index).\n",
        "\n",
        "    Returns:\n",
        "        tuple: The optimal action tuple.\n",
        "        None: If the current state is invalid (validated internally).\n",
        "    \"\"\"\n",
        "    # Validate the input state\n",
        "    if not is_valid_state(current_state, state_to_index):\n",
        "        return None # Error message printed by is_valid_state\n",
        "\n",
        "    # Get the index for the current state\n",
        "    state_index = state_to_index[current_state]\n",
        "\n",
        "    # Find the index of the action with the maximum Q-value for this state\n",
        "    optimal_action_index = np.argmax(q_table[state_index, :])\n",
        "\n",
        "    # Return the corresponding action tuple\n",
        "    return index_to_action[optimal_action_index]\n",
        "\n",
        "def get_top_n_actions_for_state(current_state: tuple, q_table: np.ndarray, state_to_index: dict, index_to_action: dict, n: int = 5) -> list | None:\n",
        "    \"\"\"\n",
        "    Retrieves the top N recommended actions and their Q-values for a given state,\n",
        "    sorted by Q-value in descending order.\n",
        "\n",
        "    Useful for understanding the policy beyond just the single optimal action.\n",
        "\n",
        "    Args:\n",
        "        current_state (tuple): The state tuple for which to find top actions.\n",
        "        q_table (np.ndarray): The learned Q-table.\n",
        "        state_to_index (dict): Mapping for state lookup (for validation and index).\n",
        "        index_to_action (dict): Mapping for action lookup (to get action tuple from index).\n",
        "        n (int): The number of top actions to return.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of tuples, where each tuple is (action_tuple, q_value),\n",
        "              sorted by Q-value (highest first).\n",
        "        None: If the current state is invalid (validated internally).\n",
        "    \"\"\"\n",
        "    # Validate the input state\n",
        "    if not is_valid_state(current_state, state_to_index):\n",
        "        return None # Error message printed by is_valid_state\n",
        "\n",
        "    # Get the index for the current state\n",
        "    state_index = state_to_index[current_state]\n",
        "\n",
        "    # Get all Q-values for this state\n",
        "    q_values = q_table[state_index, :]\n",
        "\n",
        "    # Get indices that sort Q-values in descending order\n",
        "    sorted_action_indices = np.argsort(q_values)[::-1]\n",
        "\n",
        "    # Get the top N action indices\n",
        "    top_n_action_indices = sorted_action_indices[:n]\n",
        "\n",
        "    # Create list of (action_tuple, q_value) for the top N actions\n",
        "    top_n_actions_with_q_values = [(index_to_action[i], q_values[i]) for i in top_n_action_indices]\n",
        "\n",
        "    return top_n_actions_with_q_values\n",
        "\n",
        "\n",
        "# --- Q-Learning Training Function ---\n",
        "\n",
        "def train_q_learning_agent(num_episodes: int, max_steps_per_episode: int, learning_rate: float, discount_factor: float, epsilon: float, epsilon_decay_rate: float, min_epsilon: float, state_to_index: dict, index_to_state: dict, action_to_index: dict, index_to_action: dict, q_table: np.ndarray):\n",
        "    \"\"\"\n",
        "    Runs the Q-learning training loop for a specified number of episodes.\n",
        "\n",
        "    Args:\n",
        "        num_episodes (int): Total number of learning episodes.\n",
        "        max_steps_per_episode (int): Maximum number of interactions within each episode.\n",
        "        learning_rate (float): Alpha (): How much the agent learns from each experience.\n",
        "        discount_factor (float): Gamma (): Discount rate for future rewards.\n",
        "        epsilon (float): Epsilon (): Initial probability of exploration.\n",
        "        epsilon_decay_rate (float): Rate at which epsilon decreases after each episode.\n",
        "        min_epsilon (float): Minimum value for epsilon.\n",
        "        state_to_index (dict): Mapping from state tuple to index.\n",
        "        index_to_state (dict): Mapping from index to state tuple.\n",
        "        action_to_index (dict): Mapping from action tuple to index.\n",
        "        index_to_action (dict): Mapping from index to action tuple.\n",
        "        q_table (np.ndarray): The Q-table to be trained (modified in-place).\n",
        "\n",
        "    Returns:\n",
        "        list: A list containing the total reward accumulated in each episode.\n",
        "    \"\"\"\n",
        "    total_rewards_per_episode = []\n",
        "    current_epsilon = epsilon # Use a variable for the decaying epsilon\n",
        "\n",
        "    print(f\"\\nStarting Q-Learning training for {num_episodes} episodes...\")\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        # Initialize starting state for each episode (must be a valid state)\n",
        "        starting_state = (3, 3, 0, 'A') # Example starting state\n",
        "\n",
        "        # Validate starting state\n",
        "        if not is_valid_state(starting_state, state_to_index):\n",
        "            print(f\"Episode {episode}: Invalid starting state {starting_state}. Skipping episode.\")\n",
        "            total_rewards_per_episode.append(0)\n",
        "            continue\n",
        "\n",
        "        current_state = starting_state\n",
        "        total_episode_reward = 0\n",
        "\n",
        "        # Episode loop\n",
        "        for step in range(max_steps_per_episode):\n",
        "            # Action selection\n",
        "            action = epsilon_greedy_action_selection(current_state, q_table, state_to_index, index_to_action, current_epsilon)\n",
        "\n",
        "            # Handle case where action selection failed (shouldn't happen with valid start state and valid_state check)\n",
        "            if action is None:\n",
        "               print(f\"Episode {episode}, Step {step}: Action selection failed. Ending episode.\")\n",
        "               break\n",
        "\n",
        "            # Simulate environment step\n",
        "            next_state, reward = simulate_next_state_and_reward(current_state, action, state_to_index, action_to_index)\n",
        "\n",
        "            # Handle simulation failure (e.g., invalid inputs passed internally which shouldn't occur now,\n",
        "            # or calculate_reward returning None due to invalid simulated next_state)\n",
        "            if next_state is None or reward is None:\n",
        "                 print(f\"Episode {episode}, Step {step}: Simulation or reward calculation failed. Ending episode.\")\n",
        "                 break\n",
        "\n",
        "            # Q-table update\n",
        "            update_q_table(q_table, current_state, action, reward, next_state, learning_rate, discount_factor, state_to_index, action_to_index)\n",
        "\n",
        "            # Accumulate reward\n",
        "            total_episode_reward += reward\n",
        "\n",
        "            # Transition to next state\n",
        "            current_state = next_state\n",
        "\n",
        "            # Optional: End episode early condition (e.g., task completion)\n",
        "            if current_state[2] == 1:\n",
        "               # print(f\"Episode {episode}, Step {step}: Task completed, ending episode.\")\n",
        "               pass # Keep episode running for fixed steps, or uncomment break\n",
        "               # break\n",
        "\n",
        "        # Epsilon decay after each episode\n",
        "        current_epsilon = max(min_epsilon, current_epsilon - epsilon_decay_rate)\n",
        "\n",
        "        # Store total reward for the episode\n",
        "        total_rewards_per_episode.append(total_episode_reward)\n",
        "\n",
        "        # Print periodic progress\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_episode_reward:.2f}, Epsilon: {current_epsilon:.4f}\")\n",
        "\n",
        "\n",
        "    print(\"\\nQ-Learning training finished.\")\n",
        "    return total_rewards_per_episode"
      ],
      "metadata": {
        "id": "FaFMDEzvluNk"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Engine Training and Analysis"
      ],
      "metadata": {
        "id": "b_X-Jgj-sV7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the training cell\n",
        "# --- Main Execution Block ---\n",
        "\n",
        "# Set random seed for reproducibility (optional, but good for debugging/comparison)\n",
        "# random.seed(42)\n",
        "# np.random.seed(42)\n",
        "\n",
        "\n",
        "# --- Environment Setup ---\n",
        "print(\"Setting up state space, action space, and Q-table...\")\n",
        "all_states_tuple, state_to_index, index_to_state, num_states = define_state_space()\n",
        "all_actions_tuple, action_to_index, index_to_action, num_actions = define_action_space()\n",
        "q_table = initialize_q_table(num_states, num_actions)\n",
        "print(f\"Setup complete. Total states: {num_states}, Total actions: {num_actions}, Q-table shape: {q_table.shape}\")\n",
        "\n",
        "\n",
        "# --- Training ---\n",
        "# Define training parameters\n",
        "num_episodes = 1000\n",
        "max_steps_per_episode = 100\n",
        "learning_rate = 0.1\n",
        "discount_factor = 0.9\n",
        "epsilon = 1.0             # Initial epsilon\n",
        "epsilon_decay_rate = 0.001\n",
        "min_epsilon = 0.01\n",
        "\n",
        "# Run the training loop using the function\n",
        "total_rewards_per_episode = train_q_learning_agent(\n",
        "    num_episodes, max_steps_per_episode, learning_rate, discount_factor,\n",
        "    epsilon, epsilon_decay_rate, min_epsilon,\n",
        "    state_to_index, index_to_state, action_to_index, index_to_action, q_table\n",
        ")\n",
        "\n",
        "# Execute the analysis cell\n",
        "# --- Analysis and Visualization ---\n",
        "\n",
        "print(\"\\n--- Learned Policy Examples ---\")\n",
        "\n",
        "# Select representative states to analyze the learned policy\n",
        "example_states = [\n",
        "    (3, 5, 1, 'D'),  # Mid-cognitive load, High engagement, Task Completed, Previous task D -> Next should be A\n",
        "    (1, 1, 0, 'A'),  # Low cognitive load, Low engagement, Task Not Completed, Previous task A -> Next should be B\n",
        "    (5, 3, 0, 'B'),  # High cognitive load, Mid engagement, Task Not Completed, Previous task B -> Next should be C\n",
        "    (2, 4, 1, 'C'),  # Low-ish cognitive load, High engagement, Task Completed, Previous task C -> Next should be D\n",
        "    (3, 3, 0, 'A')   # The starting state used in training, Previous task A -> Next should be B\n",
        "]\n",
        "\n",
        "# Analyze and print policy for valid example states\n",
        "valid_example_states = [state for state in example_states if is_valid_state(state, state_to_index)]\n",
        "\n",
        "if not valid_example_states:\n",
        "    print(\"No valid example states to display policy for.\")\n",
        "else:\n",
        "    for state in valid_example_states:\n",
        "        print(f\"\\n--- Policy for State: {state} ---\")\n",
        "\n",
        "        # Get and print optimal action\n",
        "        optimal_action = get_optimal_action_for_state(state, q_table, state_to_index, index_to_action)\n",
        "        if optimal_action:\n",
        "            print(f\"Optimal action: {optimal_action}\")\n",
        "        else:\n",
        "             print(\"Could not determine optimal action.\")\n",
        "\n",
        "        # Get and print top N actions\n",
        "        num_top_actions_to_show = 5\n",
        "        top_actions = get_top_n_actions_for_state(state, q_table, state_to_index, index_to_action, n=num_top_actions_to_show)\n",
        "        if top_actions:\n",
        "            print(f\"Top {num_top_actions_to_show} recommended actions and their Q-values:\")\n",
        "            for action, q_value in top_actions:\n",
        "                print(f\"  {action}: {q_value:.4f}\")\n",
        "        else:\n",
        "             print(f\"Could not determine top {num_top_actions_to_show} actions.\")\n",
        "\n",
        "# Plot the total rewards per episode learning curve\n",
        "# Create a Plotly figure object for the reward plot\n",
        "reward_fig = go.Figure()\n",
        "\n",
        "# Add a scatter trace for the total rewards per episode\n",
        "reward_fig.add_trace(go.Scatter(x=list(range(len(total_rewards_per_episode))), y=total_rewards_per_episode,\n",
        "                                mode='lines', name='Total Reward'))\n",
        "\n",
        "# Update the layout with title and axis labels\n",
        "reward_fig.update_layout(\n",
        "    title='Total Reward per Episode during Training',\n",
        "    xaxis_title='Episode',\n",
        "    yaxis_title='Total Reward'\n",
        ")\n",
        "\n",
        "# Display the Plotly figure\n",
        "reward_fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AmPY1btssZJ8",
        "outputId": "01ae06d2-6e10-470d-991f-9512f7f25428"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up state space, action space, and Q-table...\n",
            "Setup complete. Total states: 200, Total actions: 44, Q-table shape: (200, 44)\n",
            "\n",
            "Starting Q-Learning training for 1000 episodes...\n",
            "Episode 100/1000, Total Reward: 626.80, Epsilon: 0.9000\n",
            "Episode 200/1000, Total Reward: 773.60, Epsilon: 0.8000\n",
            "Episode 300/1000, Total Reward: 705.60, Epsilon: 0.7000\n",
            "Episode 400/1000, Total Reward: 856.80, Epsilon: 0.6000\n",
            "Episode 500/1000, Total Reward: 906.00, Epsilon: 0.5000\n",
            "Episode 600/1000, Total Reward: 890.80, Epsilon: 0.4000\n",
            "Episode 700/1000, Total Reward: 914.40, Epsilon: 0.3000\n",
            "Episode 800/1000, Total Reward: 887.60, Epsilon: 0.2000\n",
            "Episode 900/1000, Total Reward: 997.60, Epsilon: 0.1000\n",
            "Episode 1000/1000, Total Reward: 1004.80, Epsilon: 0.0100\n",
            "\n",
            "Q-Learning training finished.\n",
            "\n",
            "--- Learned Policy Examples ---\n",
            "\n",
            "--- Policy for State: (3, 5, 1, 'D') ---\n",
            "Optimal action: ('A', 0)\n",
            "Top 5 recommended actions and their Q-values:\n",
            "  ('A', 0): 103.7790\n",
            "  ('A', 1): 100.0323\n",
            "  ('A', 3): 99.7849\n",
            "  ('A', 5): 98.1337\n",
            "  ('A', 2): 97.5965\n",
            "\n",
            "--- Policy for State: (1, 1, 0, 'A') ---\n",
            "Optimal action: ('B', 8)\n",
            "Top 5 recommended actions and their Q-values:\n",
            "  ('B', 8): 3.9231\n",
            "  ('B', 10): 0.8030\n",
            "  ('D', 10): 0.0000\n",
            "  ('D', 9): 0.0000\n",
            "  ('D', 6): 0.0000\n",
            "\n",
            "--- Policy for State: (5, 3, 0, 'B') ---\n",
            "Optimal action: ('A', 0)\n",
            "Top 5 recommended actions and their Q-values:\n",
            "  ('D', 10): 0.0000\n",
            "  ('D', 9): 0.0000\n",
            "  ('D', 8): 0.0000\n",
            "  ('D', 7): 0.0000\n",
            "  ('D', 6): 0.0000\n",
            "\n",
            "--- Policy for State: (2, 4, 1, 'C') ---\n",
            "Optimal action: ('D', 4)\n",
            "Top 5 recommended actions and their Q-values:\n",
            "  ('D', 4): 92.6573\n",
            "  ('D', 6): 59.8068\n",
            "  ('D', 8): 49.6225\n",
            "  ('D', 5): 45.4770\n",
            "  ('D', 2): 37.9915\n",
            "\n",
            "--- Policy for State: (3, 3, 0, 'A') ---\n",
            "Optimal action: ('B', 7)\n",
            "Top 5 recommended actions and their Q-values:\n",
            "  ('B', 7): 100.3961\n",
            "  ('B', 6): 95.8653\n",
            "  ('B', 3): 94.0903\n",
            "  ('B', 4): 93.5495\n",
            "  ('B', 5): 84.3883\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"f80f3715-802c-47af-a88c-f82b6bd84bf8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f80f3715-802c-47af-a88c-f82b6bd84bf8\")) {                    Plotly.newPlot(                        \"f80f3715-802c-47af-a88c-f82b6bd84bf8\",                        [{\"mode\":\"lines\",\"name\":\"Total Reward\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999],\"y\":[748.3999999999999,557.5999999999998,708.7999999999996,581.1999999999998,629.5999999999998,604.7999999999998,589.1999999999996,620.3999999999997,584.7999999999997,545.5999999999998,629.9999999999997,611.5999999999998,591.5999999999998,616.7999999999996,548.7999999999998,651.1999999999997,580.3999999999997,627.9999999999998,627.9999999999998,640.3999999999997,655.1999999999998,706.3999999999997,500.3999999999997,604.7999999999998,696.3999999999996,741.9999999999995,544.3999999999999,459.9999999999999,665.5999999999997,601.1999999999997,624.3999999999997,694.7999999999997,583.1999999999998,491.9999999999998,481.5999999999999,535.5999999999998,624.7999999999997,468.3999999999999,595.9999999999998,230.80000000000007,624.7999999999997,654.7999999999997,603.5999999999997,606.7999999999997,624.3999999999996,550.3999999999996,576.7999999999998,635.9999999999998,600.7999999999997,545.5999999999997,539.1999999999998,475.1999999999998,567.9999999999998,386.4,581.1999999999997,567.1999999999998,412.79999999999984,573.9999999999999,456.4,461.5999999999998,535.5999999999998,657.5999999999998,687.1999999999998,626.7999999999997,529.9999999999998,593.5999999999997,545.9999999999999,596.7999999999996,599.1999999999998,525.9999999999999,597.5999999999997,590.3999999999999,599.9999999999998,507.99999999999994,601.1999999999998,525.5999999999999,607.1999999999997,566.3999999999997,598.7999999999998,579.1999999999999,597.1999999999998,597.9999999999998,493.1999999999999,580.3999999999999,660.7999999999998,434.3999999999999,501.5999999999998,574.3999999999997,661.9999999999999,591.5999999999998,608.3999999999997,574.7999999999998,652.7999999999998,400.79999999999995,565.9999999999999,628.7999999999998,668.7999999999997,607.9999999999999,677.9999999999997,626.7999999999998,648.3999999999996,649.5999999999998,762.3999999999999,761.1999999999996,611.1999999999996,638.7999999999997,593.5999999999998,633.5999999999997,591.1999999999998,647.5999999999998,676.3999999999997,582.7999999999997,580.7999999999998,688.7999999999996,643.5999999999997,673.1999999999997,725.9999999999998,571.5999999999998,689.9999999999998,721.1999999999997,612.4,693.9999999999998,709.1999999999997,705.1999999999996,616.7999999999997,637.5999999999998,634.7999999999997,697.1999999999998,642.7999999999996,554.7999999999997,635.1999999999997,689.9999999999998,666.3999999999996,675.5999999999997,637.1999999999997,697.1999999999998,657.5999999999997,659.1999999999997,786.7999999999998,661.9999999999999,636.3999999999999,701.1999999999997,737.9999999999998,617.5999999999998,541.1999999999998,735.9999999999998,713.5999999999998,672.7999999999995,711.1999999999999,680.3999999999997,695.9999999999998,652.7999999999997,614.3999999999997,639.1999999999998,692.7999999999997,660.7999999999997,707.9999999999998,671.9999999999997,672.3999999999996,657.1999999999996,467.19999999999993,663.9999999999999,741.9999999999998,701.5999999999998,568.7999999999997,669.5999999999998,669.1999999999997,785.1999999999998,659.1999999999997,707.5999999999997,664.7999999999997,741.9999999999998,749.5999999999996,755.5999999999997,648.3999999999997,752.3999999999997,629.1999999999998,809.1999999999997,729.5999999999999,756.3999999999997,683.1999999999998,531.9999999999999,673.9999999999998,555.9999999999998,727.9999999999998,566.7999999999998,660.3999999999996,647.1999999999997,680.3999999999996,591.5999999999998,725.5999999999998,701.9999999999998,571.1999999999997,727.1999999999997,668.3999999999997,657.5999999999998,720.7999999999998,666.7999999999996,460.4,773.5999999999997,659.9999999999998,727.9999999999998,366.8,618.7999999999997,642.7999999999997,637.9999999999997,759.9999999999998,704.7999999999997,676.7999999999997,722.3999999999997,683.9999999999998,701.5999999999997,696.7999999999996,721.1999999999997,721.1999999999996,754.3999999999997,421.19999999999993,707.1999999999997,662.3999999999997,702.3999999999996,750.3999999999995,768.3999999999997,649.9999999999997,656.3999999999996,723.1999999999996,674.3999999999997,786.3999999999995,665.1999999999998,838.7999999999997,763.1999999999997,753.1999999999997,711.9999999999998,740.3999999999994,674.3999999999997,669.1999999999998,680.3999999999997,627.9999999999998,727.9999999999997,532.3999999999997,760.3999999999995,647.9999999999997,693.1999999999997,765.9999999999997,655.9999999999998,697.1999999999997,683.5999999999999,741.1999999999998,650.3999999999995,759.9999999999998,729.9999999999997,704.3999999999997,649.1999999999997,708.3999999999997,656.7999999999996,719.5999999999996,821.9999999999997,705.9999999999999,822.7999999999997,824.7999999999997,811.9999999999997,725.5999999999997,775.1999999999997,752.3999999999999,689.9999999999998,796.3999999999996,785.9999999999998,679.9999999999995,831.1999999999997,743.9999999999998,730.7999999999997,334.0,799.9999999999998,754.3999999999999,749.5999999999998,791.9999999999997,783.5999999999998,686.3999999999997,691.1999999999997,692.3999999999997,671.5999999999997,794.3999999999999,859.1999999999997,784.7999999999997,777.1999999999997,754.3999999999996,819.9999999999998,759.9999999999997,700.7999999999997,601.1999999999997,745.5999999999998,883.5999999999996,574.8,773.1999999999998,654.3999999999997,644.7999999999997,816.3999999999997,663.5999999999997,757.1999999999997,806.3999999999996,705.5999999999997,776.3999999999999,816.7999999999997,779.5999999999998,718.3999999999996,742.7999999999997,792.3999999999997,719.9999999999995,843.5999999999997,775.9999999999995,739.1999999999998,794.3999999999996,713.1999999999997,758.7999999999997,720.7999999999997,788.7999999999996,751.9999999999998,749.1999999999997,704.7999999999998,837.9999999999998,722.3999999999997,712.7999999999997,762.3999999999996,823.9999999999995,778.7999999999997,881.1999999999997,733.9999999999997,826.3999999999997,751.1999999999998,760.7999999999996,735.9999999999995,745.1999999999996,824.7999999999997,834.7999999999997,801.1999999999997,709.5999999999998,780.3999999999999,764.7999999999997,877.9999999999997,882.3999999999999,789.1999999999996,737.1999999999998,814.3999999999996,756.3999999999996,737.9999999999997,622.3999999999997,821.1999999999998,736.7999999999997,819.1999999999997,729.1999999999997,757.5999999999997,853.9999999999995,746.7999999999998,628.7999999999997,767.9999999999998,745.5999999999997,755.9999999999997,773.9999999999997,751.1999999999996,769.9999999999999,789.5999999999998,663.1999999999998,736.7999999999996,817.1999999999996,832.7999999999998,761.1999999999997,761.9999999999998,811.1999999999997,758.7999999999996,789.5999999999997,756.3999999999997,771.9999999999998,851.1999999999996,715.5999999999998,795.5999999999996,803.5999999999997,830.7999999999996,784.3999999999995,821.5999999999997,666.7999999999997,874.3999999999997,831.5999999999997,799.1999999999997,834.3999999999997,763.9999999999997,816.7999999999997,801.1999999999998,762.3999999999999,852.3999999999995,829.5999999999996,806.7999999999996,745.1999999999997,747.9999999999997,778.7999999999997,655.1999999999998,799.5999999999997,806.3999999999996,794.3999999999997,758.3999999999997,825.5999999999997,856.7999999999997,789.9999999999995,765.9999999999995,865.1999999999997,852.7999999999998,686.7999999999997,827.9999999999998,826.7999999999995,812.3999999999999,805.1999999999996,833.1999999999996,810.7999999999996,789.9999999999995,820.7999999999996,794.3999999999997,823.1999999999997,784.3999999999995,864.7999999999997,842.7999999999997,817.9999999999997,778.7999999999995,650.3999999999999,860.7999999999997,825.1999999999998,865.5999999999997,871.9999999999998,878.3999999999995,839.5999999999997,795.9999999999997,789.5999999999997,777.1999999999997,797.1999999999996,819.5999999999997,822.7999999999996,783.9999999999998,879.1999999999998,801.5999999999996,860.3999999999997,859.5999999999997,808.7999999999998,889.1999999999997,754.7999999999996,846.3999999999996,861.5999999999998,802.3999999999995,829.9999999999995,839.1999999999998,765.5999999999998,851.1999999999998,851.9999999999997,799.9999999999997,874.3999999999997,820.3999999999997,782.7999999999996,789.1999999999997,844.3999999999997,786.3999999999996,853.9999999999998,790.3999999999996,810.3999999999996,758.7999999999997,865.1999999999997,773.1999999999996,887.9999999999997,853.9999999999997,890.7999999999997,809.1999999999998,818.7999999999996,753.5999999999996,876.7999999999997,867.9999999999997,870.3999999999997,825.1999999999998,825.9999999999998,835.5999999999997,910.3999999999996,769.1999999999996,802.7999999999996,811.1999999999997,862.7999999999997,819.9999999999998,837.1999999999997,827.5999999999996,859.5999999999997,850.3999999999997,899.9999999999997,808.7999999999997,824.3999999999997,893.9999999999998,854.7999999999996,901.5999999999997,939.5999999999998,721.1999999999995,753.9999999999997,883.1999999999997,838.7999999999996,796.3999999999996,863.5999999999996,913.1999999999997,873.5999999999997,905.9999999999997,825.1999999999996,823.1999999999996,834.3999999999997,883.1999999999997,812.7999999999997,921.5999999999998,859.5999999999997,798.7999999999995,827.1999999999997,921.9999999999997,810.3999999999996,811.5999999999997,812.3999999999996,917.5999999999997,883.5999999999996,869.9999999999998,858.7999999999997,882.3999999999996,873.9999999999997,813.9999999999995,894.7999999999997,861.9999999999997,876.7999999999996,903.5999999999997,905.1999999999997,833.1999999999997,843.5999999999998,881.1999999999998,823.5999999999997,817.9999999999997,936.7999999999998,923.5999999999998,913.1999999999997,852.7999999999996,916.3999999999995,868.3999999999997,884.7999999999996,902.3999999999995,877.9999999999997,875.5999999999997,847.5999999999996,803.9999999999997,806.3999999999996,899.9999999999997,908.7999999999996,836.7999999999997,884.7999999999997,817.1999999999997,871.5999999999997,941.9999999999998,869.1999999999995,874.7999999999997,864.3999999999996,847.9999999999997,870.3999999999996,867.1999999999996,789.1999999999998,884.3999999999996,869.1999999999998,897.9999999999995,889.9999999999997,926.3999999999996,869.1999999999996,944.7999999999996,865.9999999999998,953.5999999999998,882.7999999999997,853.1999999999996,909.9999999999997,827.1999999999996,920.7999999999996,897.5999999999997,861.1999999999996,897.5999999999996,863.5999999999997,843.9999999999995,805.9999999999997,917.5999999999998,752.7999999999998,841.1999999999996,845.9999999999995,935.9999999999998,905.5999999999997,690.7999999999996,885.1999999999996,905.5999999999996,897.5999999999996,906.7999999999997,799.5999999999995,913.9999999999997,903.5999999999997,922.3999999999997,985.1999999999998,869.1999999999995,846.7999999999997,872.7999999999997,873.9999999999997,847.1999999999997,935.5999999999997,890.7999999999997,859.5999999999997,873.1999999999996,877.1999999999997,876.7999999999997,885.9999999999997,890.7999999999996,891.5999999999996,836.3999999999996,881.9999999999997,882.3999999999996,892.7999999999997,836.3999999999996,762.7999999999997,906.3999999999997,868.7999999999997,873.5999999999996,901.1999999999995,910.7999999999997,909.1999999999996,853.5999999999996,938.3999999999995,938.7999999999995,933.9999999999997,885.5999999999997,911.1999999999996,877.1999999999997,909.1999999999996,836.7999999999996,953.1999999999997,915.9999999999995,850.3999999999996,946.3999999999996,894.7999999999996,923.9999999999995,886.7999999999997,910.7999999999997,969.9999999999998,921.1999999999997,909.1999999999997,932.7999999999996,891.5999999999997,799.5999999999996,933.1999999999996,869.9999999999998,923.5999999999999,908.7999999999996,884.3999999999995,909.5999999999997,901.5999999999997,879.5999999999998,868.7999999999995,985.9999999999997,919.9999999999995,882.7999999999996,877.5999999999996,907.1999999999997,855.1999999999997,905.5999999999997,925.5999999999997,855.1999999999996,909.9999999999995,907.9999999999997,923.9999999999997,913.9999999999995,912.7999999999997,917.9999999999997,911.5999999999997,931.5999999999998,961.1999999999997,847.9999999999994,887.1999999999996,797.1999999999998,933.9999999999997,925.9999999999994,892.7999999999996,978.7999999999997,944.7999999999996,865.1999999999996,959.1999999999997,913.5999999999997,932.7999999999997,886.7999999999996,926.3999999999996,947.1999999999997,954.3999999999997,912.7999999999997,883.9999999999997,965.5999999999997,930.3999999999999,932.3999999999996,927.9999999999995,943.5999999999997,933.9999999999995,892.7999999999997,971.1999999999996,908.3999999999997,956.7999999999997,922.7999999999997,875.1999999999996,914.3999999999997,877.9999999999997,951.1999999999997,913.5999999999997,928.3999999999997,887.1999999999997,899.5999999999996,941.1999999999996,936.3999999999997,935.9999999999997,980.3999999999997,945.5999999999997,956.3999999999996,854.3999999999997,887.9999999999998,917.9999999999997,895.9999999999995,923.5999999999996,954.7999999999996,928.7999999999996,927.9999999999998,945.1999999999996,933.5999999999996,928.7999999999997,980.7999999999997,933.5999999999997,906.3999999999996,889.5999999999997,982.7999999999998,913.9999999999997,965.9999999999995,931.5999999999996,976.7999999999997,967.1999999999997,928.7999999999998,920.3999999999996,911.9999999999997,975.1999999999997,953.5999999999997,948.7999999999997,921.5999999999997,949.5999999999996,938.7999999999996,947.5999999999996,969.9999999999998,977.1999999999996,971.9999999999997,895.1999999999997,933.9999999999997,973.1999999999998,883.9999999999995,938.7999999999997,975.9999999999998,919.5999999999997,867.9999999999995,948.3999999999996,941.5999999999997,952.7999999999996,969.5999999999998,963.9999999999997,935.5999999999997,958.7999999999997,984.7999999999996,921.5999999999996,942.3999999999996,925.1999999999997,950.7999999999997,928.3999999999995,935.5999999999997,957.1999999999997,936.3999999999995,893.9999999999998,963.5999999999998,961.5999999999998,1012.7999999999998,964.7999999999997,1010.7999999999998,917.5999999999996,943.5999999999996,973.9999999999998,953.5999999999998,957.9999999999997,984.3999999999996,902.3999999999996,976.7999999999995,948.7999999999997,995.9999999999997,977.1999999999997,1007.5999999999998,937.1999999999996,972.3999999999997,977.1999999999998,899.9999999999997,921.9999999999997,969.5999999999996,957.1999999999997,987.5999999999997,956.3999999999997,987.5999999999997,933.9999999999997,887.5999999999996,989.1999999999996,977.9999999999998,959.9999999999995,964.3999999999997,988.7999999999996,960.7999999999995,985.1999999999997,942.7999999999996,1002.3999999999997,935.9999999999998,905.5999999999996,751.9999999999997,930.7999999999995,982.7999999999996,973.9999999999995,966.3999999999996,937.5999999999996,983.9999999999997,957.1999999999997,907.1999999999997,967.1999999999997,977.9999999999998,991.5999999999996,938.7999999999995,1000.7999999999997,1005.9999999999997,960.3999999999995,988.7999999999997,951.1999999999997,1016.3999999999997,988.7999999999996,988.7999999999997,977.9999999999998,986.3999999999997,946.3999999999997,993.5999999999998,999.1999999999997,991.1999999999997,986.3999999999996,1000.7999999999996,992.3999999999996,966.7999999999997,971.1999999999997,961.1999999999997,959.9999999999997,984.3999999999996,988.7999999999996,999.9999999999997,991.5999999999997,1009.1999999999998,989.9999999999997,979.5999999999997,1020.3999999999997,968.3999999999997,994.7999999999997,979.9999999999997,974.7999999999996,1004.3999999999997,1010.7999999999997,968.7999999999996,964.7999999999997,989.9999999999997,997.1999999999997,973.5999999999997,998.3999999999996,963.9999999999997,967.5999999999997,979.1999999999996,974.7999999999997,985.1999999999997,998.3999999999996,985.1999999999997,995.9999999999995,1004.3999999999996,972.3999999999996,1016.3999999999997,1002.3999999999997,993.1999999999997,998.3999999999997,953.5999999999996,966.7999999999996,998.7999999999997,1007.5999999999998,953.9999999999994,971.9999999999995,1017.5999999999998,989.9999999999997,973.5999999999997,992.7999999999997,991.1999999999997,1000.7999999999996,973.5999999999997,972.3999999999995,997.1999999999997,990.3999999999996,976.7999999999995,1005.5999999999997,992.7999999999996,1009.5999999999998,997.5999999999997,1013.9999999999997,1001.1999999999997,978.3999999999996,997.1999999999996,1022.3999999999997,1001.9999999999997,1009.1999999999997,979.5999999999997,980.3999999999996,1010.7999999999996,1000.7999999999997,985.5999999999996,991.1999999999995,1016.3999999999996,1007.9999999999997,1042.7999999999997,944.3999999999996,1017.5999999999997,985.5999999999997,1003.1999999999996,1002.3999999999996,980.3999999999996,1026.3999999999996,983.1999999999995,989.9999999999998,987.5999999999997,1003.1999999999997,1018.7999999999997,1007.9999999999995,1006.7999999999998,1004.3999999999996,988.7999999999996,1005.5999999999996,1011.9999999999995,1034.3999999999996,1023.9999999999998,1016.3999999999997,1030.7999999999997,973.1999999999996,1009.1999999999997,1019.9999999999997,996.3999999999996,1002.7999999999996,1012.7999999999998,1005.5999999999997,1021.1999999999997,1024.7999999999997,997.1999999999997,1013.9999999999997,1016.3999999999997,1037.9999999999998,1016.3999999999996,1015.1999999999998,1005.5999999999995,1009.1999999999996,1009.1999999999996,1042.7999999999997,1035.9999999999995,1038.0,1010.3999999999996,1040.3999999999996,1015.5999999999997,1012.7999999999996,1034.3999999999996,1034.3999999999999,1032.0,1027.1999999999998,1012.7999999999996,1027.1999999999998,1023.5999999999997,1019.9999999999997,1018.7999999999998,1033.1999999999998,1025.9999999999995,1029.6,1012.7999999999997,1006.7999999999996,1022.3999999999997,1027.1999999999998,991.5999999999996,1037.9999999999998,1015.1999999999997,1033.1999999999998,1033.1999999999998,1025.9999999999995,1011.5999999999996,1029.6,1033.1999999999998,1024.7999999999997,1040.3999999999996,1012.7999999999997,1040.3999999999996,1011.5999999999996,1025.9999999999995,1033.1999999999998,1022.3999999999996,1033.1999999999998,1001.9999999999997,1007.9999999999997,1004.7999999999997],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Total Reward per Episode during Training\"},\"xaxis\":{\"title\":{\"text\":\"Episode\"}},\"yaxis\":{\"title\":{\"text\":\"Total Reward\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f80f3715-802c-47af-a88c-f82b6bd84bf8');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7819b054"
      },
      "source": [
        "## Monitoring and Feedback\n",
        "\n",
        "After each task, the learner's engagement rate is recorded and added to the **Shewhart Control Chart**, which maintains a rolling window of recent data. The chart continuously calculates the Central Line (mean) and Upper/Lower Control Limits ($\\pm3$ standard deviations). It checks if the latest engagement rate falls outside these limits, indicating potentially abnormal engagement. If an anomaly is detected (conceptually triggering an alert), or at specific points in the learning process, the **Feedback Interface** is activated. This interface displays the Shewhart Control Chart visualization, showing the engagement trend and limits, alongside the system's recommended next task (task type and difficulty). The interface prompts the user (e.g., a teacher or administrator) to review this information and manually adjust the recommended task difficulty if they deem it necessary. Finally, the Feedback Interface returns the final decision, which is the recommended task type and the potentially adjusted difficulty value, to override the system's original recommendation for the learner's next task."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shewhart Control Chart"
      ],
      "metadata": {
        "id": "l3VhX2mQx9SB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bfc0a0a"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go # Used for visualization\n",
        "import random # Assuming random might be used for simulating new data in tests, though not strictly in the FeedbackInterface logic itself.\n",
        "import sys # To handle input from user\n",
        "\n",
        "class ShewhartControlChart:\n",
        "    \"\"\"\n",
        "    Implements a Shewhart Control Chart for monitoring engagement rate per task.\n",
        "    Calculates Central Line (CL), Upper Control Limit (UCL), and Lower Control Limit (LCL)\n",
        "    using a rolling window of the last N tasks. Detects anomalies when engagement\n",
        "    rate falls outside the control limits.\n",
        "    \"\"\"\n",
        "    def __init__(self, window_size=10):\n",
        "        \"\"\"\n",
        "        Initializes the ShewhartControlChart.\n",
        "\n",
        "        Args:\n",
        "            window_size (int): The number of recent data points to use for calculating\n",
        "                               CL, UCL, and LCL. Defaults to 10.\n",
        "        \"\"\"\n",
        "        if not isinstance(window_size, int) or window_size <= 0:\n",
        "            raise ValueError(\"window_size must be a positive integer.\")\n",
        "\n",
        "        self.window_size = window_size\n",
        "        self._engagement_data = [] # Stores data within the window\n",
        "        self.cl = None\n",
        "        self.ucl = None\n",
        "        self.lcl = None\n",
        "        # Store indices relative to the current window data. Reset and recalculate on add_data.\n",
        "        self._anomalies = []\n",
        "\n",
        "    def add_data(self, engagement_rate: float):\n",
        "        \"\"\"\n",
        "        Adds a new engagement rate data point, maintains the rolling window,\n",
        "        and recalculates limits and anomalies for the current window.\n",
        "\n",
        "        Args:\n",
        "            engagement_rate (float): The new engagement rate value (expected to be between 0 and 1).\n",
        "        \"\"\"\n",
        "        if not isinstance(engagement_rate, (int, float)):\n",
        "             print(f\"Warning: Received non-numeric engagement rate: {engagement_rate}. Skipping.\")\n",
        "             return\n",
        "        # Allow values outside [0, 1] to be added for control chart purposes,\n",
        "        # as out-of-control points might theoretically exceed these bounds in calculation,\n",
        "        # though the LCL is clamped at 0.0.\n",
        "        # if not 0.0 <= engagement_rate <= 1.0:\n",
        "        #      print(f\"Warning: Engagement rate {engagement_rate} is outside the expected range [0, 1]. Adding anyway.\")\n",
        "\n",
        "        self._engagement_data.append(engagement_rate)\n",
        "        # Maintain rolling window size\n",
        "        if len(self._engagement_data) > self.window_size:\n",
        "            self._engagement_data.pop(0) # Remove the oldest data point\n",
        "\n",
        "        # Always recalculate limits and anomalies after adding data\n",
        "        self.calculate_limits()\n",
        "\n",
        "    def calculate_limits(self):\n",
        "        \"\"\"\n",
        "        Calculates and updates the Central Line (CL), Upper Control Limit (UCL),\n",
        "        and Lower Control Limit (LCL) based on the data in the rolling window.\n",
        "        Also updates the list of anomalies within the current window.\n",
        "        Requires at least 2 data points in the window to calculate standard deviation.\n",
        "        \"\"\"\n",
        "        if len(self._engagement_data) < 2:\n",
        "            self.cl = None\n",
        "            self.ucl = None\n",
        "            self.lcl = None\n",
        "            self._anomalies = [] # Clear anomalies if not enough data\n",
        "            return\n",
        "\n",
        "        # Calculate based on the data currently in the window\n",
        "        data_window = np.array(self._engagement_data)\n",
        "\n",
        "        self.cl = np.mean(data_window)\n",
        "        # Using population std dev (default for np.std) is common for control charts with fixed subgroups.\n",
        "        # For a rolling window of individual measurements, sample std dev (ddof=1) might be argued,\n",
        "        # but 3-sigma limits often use this approach. Let's keep np.std default.\n",
        "        std_dev = np.std(data_window)\n",
        "\n",
        "        self.ucl = self.cl + 3 * std_dev\n",
        "        self.lcl = self.cl - 3 * std_dev\n",
        "\n",
        "        # Ensure LCL is not below 0 for engagement rate (which is 0-1)\n",
        "        # and UCL is not above 1.0 (as engagement is 0-1).\n",
        "        # Clamping limits is standard practice for bounded metrics.\n",
        "        self.lcl = max(0.0, self.lcl)\n",
        "        self.ucl = min(1.0, self.ucl)\n",
        "\n",
        "\n",
        "        # Update anomalies based on the *current* window data and the *newly calculated* limits\n",
        "        # This stores the indices within the current window that are anomalous\n",
        "        # This seems correct for visualization.\n",
        "        self._anomalies = [i for i, rate in enumerate(self._engagement_data)\n",
        "                           if rate > self.ucl or rate < self.lcl]\n",
        "\n",
        "\n",
        "    def check_for_anomaly(self) -> bool:\n",
        "        \"\"\"\n",
        "        Checks if the latest data point added to the chart is an anomaly\n",
        "        based on the current control limits. The latest point is the last element\n",
        "        in the internal data list.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the latest rate is an anomaly (outside UCL/LCL),\n",
        "                  False otherwise. Returns False if not enough data for limits.\n",
        "        \"\"\"\n",
        "        # Ensure limits have been calculated and there's at least one data point in the window\n",
        "        if self.cl is None or len(self._engagement_data) == 0:\n",
        "            return False\n",
        "\n",
        "        latest_engagement_rate = self._engagement_data[-1]\n",
        "\n",
        "        # Directly compare the latest rate to the current limits.\n",
        "        # This logic should be correct. Let's add debug prints again to see the values.\n",
        "        # print(f\"DEBUG check_for_anomaly: Latest rate = {latest_engagement_rate:.4f}, UCL = {self.ucl:.4f}, LCL = {self.lcl:.4f}\")\n",
        "        return latest_engagement_rate > self.ucl or latest_engagement_rate < self.lcl\n",
        "\n",
        "\n",
        "    def get_chart_data(self) -> dict:\n",
        "        \"\"\"\n",
        "        Returns the current chart data including engagement rates and control limits.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing:\n",
        "                  - 'engagement_rates': List of engagement rates in the current window.\n",
        "                  - 'cl': Central Line value.\n",
        "                  - 'ucl': Upper Control Limit value.\n",
        "                  - 'lcl': Lower Control Limit value.\n",
        "                  - 'anomalies': List of indices (relative to the current window data) of anomalous points.\n",
        "                  Returns None if there is not enough data to calculate limits.\n",
        "        \"\"\"\n",
        "        if self.cl is None or self.ucl is None or self.lcl is None:\n",
        "            print(\"Not enough data to get chart data.\")\n",
        "            return None\n",
        "\n",
        "        return {\n",
        "            'engagement_rates': self._engagement_data,\n",
        "            'cl': self.cl,\n",
        "            'ucl': self.ucl,\n",
        "            'lcl': self.lcl,\n",
        "            'anomalies': self._anomalies\n",
        "        }\n",
        "\n",
        "\n",
        "    def visualize_chart(self):\n",
        "        \"\"\"\n",
        "        Creates a Plotly line chart of the engagement rate over time (within the window),\n",
        "        including lines for CL, UCL, and LCL, and highlights anomalies.\n",
        "\n",
        "        Returns:\n",
        "            plotly.graph_objects.Figure or None: The Plotly figure object, or None if\n",
        "                                        there is not enough data to visualize.\n",
        "        \"\"\"\n",
        "        chart_data = self.get_chart_data()\n",
        "        if chart_data is None:\n",
        "            print(\"Not enough data to visualize the chart.\")\n",
        "            return None\n",
        "\n",
        "        engagement_rates = chart_data['engagement_rates']\n",
        "        cl = chart_data['cl']\n",
        "        ucl = chart_data['ucl']\n",
        "        lcl = chart_data['lcl']\n",
        "        anomalies = chart_data['anomalies']\n",
        "\n",
        "        fig = go.Figure()\n",
        "\n",
        "        # Add engagement rate data\n",
        "        fig.add_trace(go.Scatter(x=list(range(len(engagement_rates))), y=engagement_rates,\n",
        "                                 mode='lines+markers', name='Engagement Rate'))\n",
        "\n",
        "        # Add CL, UCL, LCL lines\n",
        "        x_range = list(range(len(engagement_rates)))\n",
        "        fig.add_trace(go.Scatter(x=x_range, y=[cl] * len(engagement_rates),\n",
        "                                 mode='lines', line=dict(dash='dash', color='green'), name='Central Line (CL)'))\n",
        "        fig.add_trace(go.Scatter(x=x_range, y=[ucl] * len(engagement_rates),\n",
        "                                 mode='lines', line=dict(dash='dash', color='red'), name='Upper Control Limit (UCL)'))\n",
        "        fig.add_trace(go.Scatter(x=x_range, y=[lcl] * len(engagement_rates),\n",
        "                                 mode='lines', line=dict(dash='dash', color='red'), name='Lower Control Limit (LCL)'))\n",
        "\n",
        "        # Highlight anomalies\n",
        "        # Use the indices stored in _anomalies, which are relative to the current window\n",
        "        anomaly_x = [i for i in anomalies]\n",
        "        anomaly_y = [engagement_rates[i] for i in anomalies] # Get the actual values at those indices\n",
        "        if anomaly_x: # Only add if there are anomalies\n",
        "             fig.add_trace(go.Scatter(x=anomaly_x, y=anomaly_y, mode='markers',\n",
        "                                     marker=dict(symbol='x', color='red', size=10),\n",
        "                                     name='Anomaly'))\n",
        "\n",
        "\n",
        "        # Update layout\n",
        "        fig.update_layout(\n",
        "            title=f'Shewhart Control Chart (Rolling Window {self.window_size})',\n",
        "            xaxis_title='Task Index (within window)',\n",
        "            yaxis_title='Engagement Rate',\n",
        "            legend=dict(\n",
        "                orientation=\"h\",\n",
        "                yanchor=\"bottom\",\n",
        "                y=1.02,\n",
        "                xanchor=\"right\",\n",
        "                x=1\n",
        "            )\n",
        "        )\n",
        "\n",
        "        return fig # Return the figure object for display"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14a7930b"
      },
      "source": [
        "### Feedback Interface Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10b9ed13"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "import sys # To handle input from user\n",
        "\n",
        "# Assuming ShewhartControlChart is defined in a previous cell\n",
        "# from .control_chart_module import ShewhartControlChart # This relative import won't work here\n",
        "\n",
        "class FeedbackInterface:\n",
        "    \"\"\"\n",
        "    Handles the interface for displaying control chart, system recommendations,\n",
        "    and allowing manual adjustment of the recommended task difficulty by the user.\n",
        "    \"\"\"\n",
        "    def __init__(self, control_chart: ShewhartControlChart):\n",
        "        \"\"\"\n",
        "        Initializes the FeedbackInterface.\n",
        "\n",
        "        Args:\n",
        "            control_chart (ShewhartControlChart): An instance of the\n",
        "                                                 ShewhartControlChart to display.\n",
        "        \"\"\"\n",
        "        if not isinstance(control_chart, ShewhartControlChart):\n",
        "            raise TypeError(\"control_chart must be an instance of ShewhartControlChart.\")\n",
        "        self.control_chart = control_chart\n",
        "\n",
        "    def get_adjusted_decision(self, current_state: tuple, recommended_action: tuple) -> tuple:\n",
        "        \"\"\"\n",
        "        Displays the control chart and recommended action, prompts the user for\n",
        "        difficulty adjustment, validates input, and returns the adjusted decision.\n",
        "\n",
        "        Args:\n",
        "            current_state (tuple): The current state of the learner (for context).\n",
        "            recommended_action (tuple): The action (task_type, difficulty) recommended\n",
        "                                        by the Q-learning agent.\n",
        "\n",
        "        Returns:\n",
        "            tuple: The adjusted action (task_type, adjusted_difficulty).\n",
        "            plotly.graph_objects.Figure or None: The generated Plotly figure, or None if not enough data.\n",
        "        \"\"\"\n",
        "        # 1. Display the current control chart\n",
        "        print(\"\\n--- Engagement Control Chart ---\")\n",
        "        # Note: The Plotly chart might not display interactively in all environments.\n",
        "        # The figure object is still created and can be saved or inspected.\n",
        "        chart_fig = self.control_chart.visualize_chart()\n",
        "        if chart_fig:\n",
        "            # Instead of just printing, we will return the figure for display in the calling code\n",
        "            print(\"Plotly chart data generated.\")\n",
        "        else:\n",
        "            print(\"Not enough data to display the control chart.\")\n",
        "\n",
        "\n",
        "        # 2. Present system's recommended action\n",
        "        recommended_task_type, recommended_difficulty = recommended_action\n",
        "        print(f\"\\nSystem Recommendation:\")\n",
        "        print(f\"  Task Type: {recommended_task_type}\")\n",
        "        print(f\"  Difficulty: {recommended_difficulty}\")\n",
        "        print(f\"  Current State: {current_state}\")\n",
        "\n",
        "        # 3. Provide mechanism for user adjustment\n",
        "        while True:\n",
        "            try:\n",
        "                user_input = input(f\"Enter desired difficulty (0-10) or press Enter to accept recommendation [{recommended_difficulty}]: \")\n",
        "                if user_input == \"\":\n",
        "                    adjusted_difficulty = recommended_difficulty\n",
        "                    print(\"Accepted recommended difficulty.\")\n",
        "                    break\n",
        "                adjusted_difficulty = int(user_input)\n",
        "                # 4. Validate user's input\n",
        "                if 0 <= adjusted_difficulty <= 10:\n",
        "                    print(f\"User adjusted difficulty to {adjusted_difficulty}.\")\n",
        "                    break\n",
        "                else:\n",
        "                    print(\"Invalid input. Difficulty must be an integer between 0 and 10.\")\n",
        "            except ValueError:\n",
        "                print(\"Invalid input. Please enter an integer or press Enter.\")\n",
        "            except EOFError: # Handle potential issues in non-interactive environments\n",
        "                 print(\"\\nEOF encountered. Using recommended difficulty.\")\n",
        "                 adjusted_difficulty = recommended_difficulty\n",
        "                 break\n",
        "            except Exception as e:\n",
        "                 print(f\"An unexpected error occurred during input: {e}. Using recommended difficulty.\")\n",
        "                 adjusted_difficulty = recommended_difficulty\n",
        "                 break\n",
        "\n",
        "\n",
        "        # 5. Return the adjusted action and the chart figure\n",
        "        adjusted_action = (recommended_task_type, adjusted_difficulty)\n",
        "        print(f\"Returning adjusted decision: {adjusted_action}\")\n",
        "        # Return both the adjusted action and the chart figure\n",
        "        return adjusted_action, chart_fig"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a87a7537"
      },
      "source": [
        "### Test Shewhart Control Chart and Feedback Interface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        },
        "id": "38e2a864",
        "outputId": "880621df-fcc0-42e9-f036-73f62c04d139"
      },
      "source": [
        "# Instantiate a ShewhartControlChart object with a window size of 10\n",
        "control_chart = ShewhartControlChart(window_size=10)\n",
        "\n",
        "# Instantiate a FeedbackInterface object, passing the created control chart instance\n",
        "feedback_interface = FeedbackInterface(control_chart=control_chart)\n",
        "\n",
        "# 1. Populate the control chart with sample data (at least 10 for window_size=10)\n",
        "sample_engagement_history = [0.75, 0.78, 0.80, 0.76, 0.82, 0.79, 0.81, 0.77, 0.83, 0.79]\n",
        "\n",
        "print(f\"Populating control chart with {len(sample_engagement_history)} sample data points...\")\n",
        "for rate in sample_engagement_history:\n",
        "    control_chart.add_data(rate)\n",
        "\n",
        "print(\"Control chart populated. Initial limits calculated.\")\n",
        "print(f\"CL: {control_chart.cl:.4f}, UCL: {control_chart.ucl:.4f}, LCL: {control_chart.lcl:.4f}\")\n",
        "\n",
        "# 2. Add a new engagement rate data point that is expected to be an anomaly\n",
        "# Based on the sample data (mean around 0.79, std dev around 0.025), LCL is ~0.7165\n",
        "# A value like 0.6 should be an anomaly.\n",
        "new_engagement_rate = 0.6\n",
        "\n",
        "# Add this new engagement rate to the ShewhartControlChart instance\n",
        "control_chart.add_data(new_engagement_rate)\n",
        "print(f\"\\nAdded new engagement rate: {new_engagement_rate}\")\n",
        "\n",
        "# 3. Check if the latest data point added is an anomaly and print an alert\n",
        "is_anomaly = control_chart.check_for_anomaly()\n",
        "\n",
        "if is_anomaly:\n",
        "    print(f\"ALERT: Anomaly detected! Latest engagement rate ({new_engagement_rate:.2f}) is outside control limits.\")\n",
        "else:\n",
        "    print(f\"Latest engagement rate ({new_engagement_rate:.2f}) is within control limits.\")\n",
        "\n",
        "# 4. Simulate getting a recommended action from the Q-Learning engine.\n",
        "simulated_current_state = (3, 4, 1, 'B') # Example state\n",
        "simulated_recommended_action = ('C', 6) # Example recommended action (Task C, Difficulty 6)\n",
        "print(f\"\\nSimulated current state: {simulated_current_state}\")\n",
        "print(f\"Simulated recommended action from Q-Learning: {simulated_recommended_action}\")\n",
        "\n",
        "# 5. Call the get_adjusted_decision() method of the FeedbackInterface instance\n",
        "# This will now display the populated chart and prompt for user input.\n",
        "print(\"\\n--- Engagement Control Chart (via FeedbackInterface) ---\")\n",
        "# Capture both the adjusted decision and the chart figure returned by the method\n",
        "adjusted_decision, chart_fig = feedback_interface.get_adjusted_decision(simulated_current_state, simulated_recommended_action)\n",
        "\n",
        "# Display the chart figure if it was generated\n",
        "if chart_fig:\n",
        "    chart_fig.show()\n",
        "\n",
        "\n",
        "# 6. Print the adjusted decision obtained from the user interface.\n",
        "print(f\"\\nFinal Adjusted Decision after Feedback: {adjusted_decision}\")"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating control chart with 10 sample data points...\n",
            "Control chart populated. Initial limits calculated.\n",
            "CL: 0.7900, UCL: 0.8635, LCL: 0.7165\n",
            "\n",
            "Added new engagement rate: 0.6\n",
            "Latest engagement rate (0.60) is within control limits.\n",
            "\n",
            "Simulated current state: (3, 4, 1, 'B')\n",
            "Simulated recommended action from Q-Learning: ('C', 6)\n",
            "\n",
            "--- Engagement Control Chart (via FeedbackInterface) ---\n",
            "\n",
            "--- Engagement Control Chart ---\n",
            "Plotly chart data generated.\n",
            "\n",
            "System Recommendation:\n",
            "  Task Type: C\n",
            "  Difficulty: 6\n",
            "  Current State: (3, 4, 1, 'B')\n",
            "Enter desired difficulty (0-10) or press Enter to accept recommendation [6]: 3\n",
            "User adjusted difficulty to 3.\n",
            "Returning adjusted decision: ('C', 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"386045f1-cad8-427f-bc5c-f9210f45ed39\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"386045f1-cad8-427f-bc5c-f9210f45ed39\")) {                    Plotly.newPlot(                        \"386045f1-cad8-427f-bc5c-f9210f45ed39\",                        [{\"mode\":\"lines+markers\",\"name\":\"Engagement Rate\",\"x\":[0,1,2,3,4,5,6,7,8,9],\"y\":[0.78,0.8,0.76,0.82,0.79,0.81,0.77,0.83,0.79,0.6],\"type\":\"scatter\"},{\"line\":{\"color\":\"green\",\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"Central Line (CL)\",\"x\":[0,1,2,3,4,5,6,7,8,9],\"y\":[0.775,0.775,0.775,0.775,0.775,0.775,0.775,0.775,0.775,0.775],\"type\":\"scatter\"},{\"line\":{\"color\":\"red\",\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"Upper Control Limit (UCL)\",\"x\":[0,1,2,3,4,5,6,7,8,9],\"y\":[0.9605397531527948,0.9605397531527948,0.9605397531527948,0.9605397531527948,0.9605397531527948,0.9605397531527948,0.9605397531527948,0.9605397531527948,0.9605397531527948,0.9605397531527948],\"type\":\"scatter\"},{\"line\":{\"color\":\"red\",\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"Lower Control Limit (LCL)\",\"x\":[0,1,2,3,4,5,6,7,8,9],\"y\":[0.5894602468472052,0.5894602468472052,0.5894602468472052,0.5894602468472052,0.5894602468472052,0.5894602468472052,0.5894602468472052,0.5894602468472052,0.5894602468472052,0.5894602468472052],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"legend\":{\"orientation\":\"h\",\"yanchor\":\"bottom\",\"y\":1.02,\"xanchor\":\"right\",\"x\":1},\"title\":{\"text\":\"Shewhart Control Chart (Rolling Window 10)\"},\"xaxis\":{\"title\":{\"text\":\"Task Index (within window)\"}},\"yaxis\":{\"title\":{\"text\":\"Engagement Rate\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('386045f1-cad8-427f-bc5c-f9210f45ed39');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Adjusted Decision after Feedback: ('C', 3)\n"
          ]
        }
      ]
    }
  ]
}